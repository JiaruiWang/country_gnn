Epoch: 001, Loss: 1.2464846373
Epoch: 002, Loss: 0.6600345373
Epoch: 003, Loss: 0.6294636130
Epoch: 004, Loss: 0.6134484410
Epoch: 005, Loss: 0.6001157761
Epoch: 006, Loss: 0.5922803879
Epoch: 007, Loss: 0.5828387737
Epoch: 008, Loss: 0.5778099895
Epoch: 009, Loss: 0.5751113296
Epoch: 010, Loss: 0.5703741312
Epoch: 011, Loss: 0.5677744150
Epoch: 012, Loss: 0.5647724271
Epoch: 013, Loss: 0.5613068342
Epoch: 014, Loss: 0.5608682632
Epoch: 015, Loss: 0.5581744909
Epoch: 016, Loss: 0.5553836226
Epoch: 017, Loss: 0.5562491417
Epoch: 018, Loss: 0.5534922481
Epoch: 019, Loss: 0.5518857837
Epoch: 020, Loss: 0.5474919081
Epoch: 021, Loss: 0.5474452376
Epoch: 022, Loss: 0.5484322309
Epoch: 023, Loss: 0.5485787392
Epoch: 024, Loss: 0.5476781726
Epoch: 025, Loss: 0.5443319082
Epoch: 026, Loss: 0.5439755917
Epoch: 027, Loss: 0.5436226130
Epoch: 028, Loss: 0.5435802937
Epoch: 029, Loss: 0.5403577089
Epoch: 030, Loss: 0.5405349135
Epoch: 031, Loss: 0.5416979790
Epoch: 032, Loss: 0.5404767990
Epoch: 033, Loss: 0.5396133661
Epoch: 034, Loss: 0.5388082266
Epoch: 035, Loss: 0.5378084183
Epoch: 036, Loss: 0.5353001356
Epoch: 037, Loss: 0.5371029973
Epoch: 038, Loss: 0.5351210833
Epoch: 039, Loss: 0.5359297991
Epoch: 040, Loss: 0.5345314145
Epoch: 041, Loss: 0.5357336402
Epoch: 042, Loss: 0.5344025493
Epoch: 043, Loss: 0.5340801477
Epoch: 044, Loss: 0.5322086811
Epoch: 045, Loss: 0.5319863558
Epoch: 046, Loss: 0.5320709944
Epoch: 047, Loss: 0.5302876234
Epoch: 048, Loss: 0.5306666493
Epoch: 049, Loss: 0.5278624296
Epoch: 050, Loss: 0.5307290554
100%|██████████| 3641/3641 [04:33<00:00, 13.31it/s]
              precision    recall  f1-score   support

           0       0.79      0.84      0.81      2420
           1       0.84      0.90      0.87      1198
           2       0.86      0.86      0.86      5436
           3       0.60      0.83      0.69      1061
           4       0.91      0.83      0.87     38476
           5       0.82      0.86      0.84      4278
           6       0.68      0.84      0.75      1174
           7       0.64      0.84      0.73       309
           8       0.91      0.86      0.88     20454
           9       0.77      0.84      0.80      3969
          10       0.89      0.88      0.88      2540
          11       0.88      0.87      0.87      2336
          12       0.87      0.84      0.86     11612
          13       0.86      0.87      0.87      3897
          14       0.84      0.85      0.84      2409
          15       0.79      0.87      0.83      2358
          16       0.65      0.79      0.71      1174
          17       0.91      0.86      0.88      4787
          18       0.82      0.91      0.86      1400
          19       0.77      0.79      0.78      2999
          20       0.85      0.87      0.86      4476
          21       0.88      0.89      0.89      6083
          22       0.76      0.86      0.80      2643
          23       0.79      0.81      0.80      1383
          24       0.70      0.81      0.75      2033
          25       0.90      0.89      0.90      1626
          26       0.57      0.82      0.67       506
          27       0.21      0.79      0.33       417
          28       0.70      0.85      0.77       835
          29       0.80      0.84      0.82      6596
          30       0.87      0.87      0.87      2194
          31       0.86      0.80      0.83     19516
          32       0.83      0.86      0.85      5071
          33       0.66      0.87      0.75       462
          34       0.70      0.83      0.76      2889
          35       0.86      0.83      0.85      3668
          36       0.73      0.87      0.79      2430
          37       0.86      0.84      0.85      8419
          38       0.73      0.86      0.79       790
          39       0.84      0.84      0.84      2877
          40       0.84      0.86      0.85       854
          41       0.70      0.80      0.75      2016
          42       0.88      0.85      0.86     16701
          43       0.79      0.85      0.82      2775
          44       0.78      0.88      0.83       755
          45       0.79      0.82      0.81      3497
          46       0.90      0.87      0.89     10146
          47       0.43      0.76      0.55       528
          48       0.50      0.77      0.61       616
          49       0.89      0.90      0.89      5205
          50       0.63      0.78      0.70       706

    accuracy                           0.84    233000
   macro avg       0.77      0.84      0.80    233000
weighted avg       0.85      0.84      0.85    233000

196581 233000
Train: 0.0000, Val: 0.0000, Test: 0.8437
100%|██████████| 36435/36435 [39:34<00:00, 15.34it/s] 
              precision    recall  f1-score   support

           0       0.60      0.85      0.71     24599
           1       0.80      0.86      0.83     12763
           2       0.89      0.61      0.72     54937
           3       0.48      0.75      0.59     10564
           4       0.94      0.78      0.85    384023
           5       0.85      0.86      0.86     42691
           6       0.48      0.85      0.61     12323
           7       0.14      0.82      0.24      3198
           8       0.84      0.78      0.81    203174
           9       0.81      0.79      0.80     40449
          10       0.72      0.18      0.29     25740
          11       0.74      0.82      0.78     23468
          12       0.82      0.42      0.56    116315
          13       0.90      0.71      0.79     39880
          14       0.44      0.79      0.56     24718
          15       0.85      0.87      0.86     22708
          16       0.39      0.75      0.52     10779
          17       0.92      0.80      0.85     47653
          18       0.59      0.92      0.72     14011
          19       0.50      0.55      0.52     28544
          20       0.91      0.90      0.91     44598
          21       0.91      0.76      0.83     62038
          22       0.81      0.82      0.82     26538
          23       0.29      0.81      0.43     14525
          24       0.62      0.63      0.62     20448
          25       0.88      0.91      0.89     16207
          26       0.29      0.86      0.44      5214
          27       0.13      0.86      0.23      4210
          28       0.65      0.72      0.69      8097
          29       0.84      0.82      0.83     65243
          30       0.77      0.84      0.80     22450
          31       0.86      0.75      0.80    195249
          32       0.83      0.76      0.79     50025
          33       0.10      0.83      0.18      4631
          34       0.74      0.64      0.69     28526
          35       0.92      0.83      0.87     36369
          36       0.77      0.83      0.80     23656
          37       0.80      0.81      0.81     85544
          38       0.78      0.76      0.77      7659
          39       0.90      0.81      0.85     29080
          40       0.63      0.61      0.62      8619
          41       0.80      0.82      0.81     19718
          42       0.87      0.64      0.74    165939
          43       0.72      0.71      0.71     27732
          44       0.68      0.85      0.76      7864
          45       0.88      0.77      0.82     35347
          46       0.89      0.88      0.88    102599
          47       0.06      0.41      0.10      5200
          48       0.07      0.78      0.13      6418
          49       0.91      0.78      0.84     52617
          50       0.22      0.74      0.34      6890

    accuracy                           0.75   2331787
   macro avg       0.67      0.76      0.67   2331787
weighted avg       0.83      0.75      0.77   2331787

1740581 2331787
Total labeled: 0.7465, Val: 0.0000, Test: 0.0000
Epoch: 051, Loss: 0.5306431055
Epoch: 052, Loss: 0.5283601880
Epoch: 053, Loss: 0.5286573172
Epoch: 054, Loss: 0.5264459252
Epoch: 055, Loss: 0.5276910067
Epoch: 056, Loss: 0.5264890194
Epoch: 057, Loss: 0.5282705426
Epoch: 058, Loss: 0.5261940360
Epoch: 059, Loss: 0.5255094767
Epoch: 060, Loss: 0.5261893868
Epoch: 061, Loss: 0.5255941153
Epoch: 062, Loss: 0.5253990889
Epoch: 063, Loss: 0.5257487297
Epoch: 064, Loss: 0.5231543779
Epoch: 065, Loss: 0.5250154734
Epoch: 066, Loss: 0.5247803330
Epoch: 067, Loss: 0.5242352486
Epoch: 068, Loss: 0.5218082666
Epoch: 069, Loss: 0.5237197876
Epoch: 070, Loss: 0.5227200389
Epoch: 071, Loss: 0.5229034424
Epoch: 072, Loss: 0.5214777589
Epoch: 073, Loss: 0.5230381489
Epoch: 074, Loss: 0.5214695930
Epoch: 075, Loss: 0.5215782523
Epoch: 076, Loss: 0.5219221115
Epoch: 077, Loss: 0.5217498541
Epoch: 078, Loss: 0.5213568807
Epoch: 079, Loss: 0.5203111172
Epoch: 080, Loss: 0.5220128298
Epoch: 081, Loss: 0.5201356411
Epoch: 082, Loss: 0.5208088756
Epoch: 083, Loss: 0.5196688771
Epoch: 084, Loss: 0.5184127688
Epoch: 085, Loss: 0.5211818814
Epoch: 086, Loss: 0.5192885995
Epoch: 087, Loss: 0.5178310871
Epoch: 088, Loss: 0.5193718672
Epoch: 089, Loss: 0.5184769630
Epoch: 090, Loss: 0.5210102201
Epoch: 091, Loss: 0.5200640559
Epoch: 092, Loss: 0.5185386539
Epoch: 093, Loss: 0.5199179053
Epoch: 094, Loss: 0.5190538168
Epoch: 095, Loss: 0.5195543170
Epoch: 096, Loss: 0.5184562802
Epoch: 097, Loss: 0.5176998377
Epoch: 098, Loss: 0.5209979415
Epoch: 099, Loss: 0.5186343789
Epoch: 100, Loss: 0.5194275975
100%|██████████| 3641/3641 [04:00<00:00, 15.15it/s]
              precision    recall  f1-score   support

           0       0.80      0.83      0.82      2420
           1       0.84      0.91      0.87      1198
           2       0.84      0.87      0.86      5436
           3       0.60      0.83      0.69      1061
           4       0.93      0.81      0.87     38476
           5       0.83      0.85      0.84      4278
           6       0.64      0.84      0.73      1174
           7       0.57      0.84      0.68       309
           8       0.91      0.86      0.88     20454
           9       0.77      0.85      0.80      3969
          10       0.87      0.88      0.87      2540
          11       0.84      0.88      0.86      2336
          12       0.86      0.84      0.85     11612
          13       0.84      0.87      0.86      3897
          14       0.84      0.85      0.84      2409
          15       0.79      0.87      0.83      2358
          16       0.61      0.79      0.69      1174
          17       0.87      0.86      0.87      4787
          18       0.83      0.91      0.87      1400
          19       0.76      0.79      0.77      2999
          20       0.84      0.87      0.86      4476
          21       0.88      0.89      0.89      6083
          22       0.77      0.85      0.81      2643
          23       0.74      0.82      0.78      1383
          24       0.69      0.81      0.75      2033
          25       0.88      0.89      0.89      1626
          26       0.62      0.82      0.71       506
          27       0.25      0.79      0.38       417
          28       0.72      0.86      0.78       835
          29       0.84      0.83      0.83      6596
          30       0.87      0.87      0.87      2194
          31       0.85      0.81      0.83     19516
          32       0.84      0.87      0.85      5071
          33       0.64      0.88      0.74       462
          34       0.71      0.83      0.76      2889
          35       0.87      0.84      0.85      3668
          36       0.77      0.86      0.81      2430
          37       0.86      0.84      0.85      8419
          38       0.74      0.86      0.79       790
          39       0.81      0.84      0.83      2877
          40       0.85      0.86      0.86       854
          41       0.72      0.79      0.76      2016
          42       0.88      0.85      0.86     16701
          43       0.76      0.86      0.81      2775
          44       0.75      0.89      0.81       755
          45       0.80      0.82      0.81      3497
          46       0.91      0.87      0.89     10146
          47       0.39      0.79      0.52       528
          48       0.48      0.79      0.60       616
          49       0.89      0.90      0.89      5205
          50       0.56      0.80      0.66       706

    accuracy                           0.84    233000
   macro avg       0.77      0.85      0.80    233000
weighted avg       0.85      0.84      0.85    233000

196425 233000
Train: 0.0000, Val: 0.0000, Test: 0.8430
100%|██████████| 36435/36435 [39:24<00:00, 15.41it/s] 
              precision    recall  f1-score   support

           0       0.81      0.84      0.82     24599
           1       0.84      0.90      0.87     12763
           2       0.83      0.87      0.85     54937
           3       0.55      0.83      0.66     10564
           4       0.93      0.76      0.84    384023
           5       0.84      0.86      0.85     42691
           6       0.63      0.85      0.73     12323
           7       0.40      0.85      0.55      3198
           8       0.91      0.85      0.88    203174
           9       0.78      0.83      0.81     40449
          10       0.87      0.88      0.87     25740
          11       0.80      0.88      0.83     23468
          12       0.88      0.83      0.85    116315
          13       0.85      0.86      0.86     39880
          14       0.82      0.85      0.84     24718
          15       0.78      0.87      0.82     22708
          16       0.50      0.80      0.61     10779
          17       0.88      0.86      0.87     47653
          18       0.84      0.91      0.87     14011
          19       0.74      0.80      0.77     28544
          20       0.84      0.87      0.86     44598
          21       0.88      0.89      0.88     62038
          22       0.77      0.85      0.81     26538
          23       0.74      0.82      0.77     14525
          24       0.70      0.81      0.75     20448
          25       0.87      0.91      0.89     16207
          26       0.53      0.85      0.65      5214
          27       0.20      0.81      0.32      4210
          28       0.67      0.86      0.76      8097
          29       0.84      0.81      0.83     65243
          30       0.85      0.86      0.86     22450
          31       0.82      0.80      0.81    195249
          32       0.83      0.86      0.85     50025
          33       0.52      0.89      0.66      4631
          34       0.72      0.82      0.77     28526
          35       0.85      0.84      0.85     36369
          36       0.76      0.86      0.81     23656
          37       0.87      0.83      0.85     85544
          38       0.69      0.87      0.77      7659
          39       0.82      0.84      0.83     29080
          40       0.82      0.85      0.84      8619
          41       0.71      0.80      0.76     19718
          42       0.90      0.83      0.86    165939
          43       0.77      0.84      0.81     27732
          44       0.75      0.89      0.81      7864
          45       0.81      0.82      0.82     35347
          46       0.92      0.86      0.89    102599
          47       0.29      0.76      0.42      5200
          48       0.32      0.79      0.46      6418
          49       0.89      0.90      0.89     52617
          50       0.21      0.79      0.33      6890

    accuracy                           0.83   2331787
   macro avg       0.74      0.84      0.77   2331787
weighted avg       0.85      0.83      0.84   2331787

1931763 2331787
Total labeled: 0.8284, Val: 0.0000, Test: 0.0000
Epoch: 101, Loss: 0.5182429552
Epoch: 102, Loss: 0.5171854496
Epoch: 103, Loss: 0.5169349909
Epoch: 104, Loss: 0.5198349953
Epoch: 105, Loss: 0.5180295110
Epoch: 106, Loss: 0.5188615322
Epoch: 107, Loss: 0.5171416402
Epoch: 108, Loss: 0.5180316567
Epoch: 109, Loss: 0.5171150565
Epoch: 110, Loss: 0.5176704526
Epoch: 111, Loss: 0.5180172920
Epoch: 112, Loss: 0.5182527900
Epoch: 113, Loss: 0.5187321305
Epoch: 114, Loss: 0.5185358524
Epoch: 115, Loss: 0.5169113278
Epoch: 116, Loss: 0.5191783309
Epoch: 117, Loss: 0.5192884803
Epoch: 118, Loss: 0.5174248219
Epoch: 119, Loss: 0.5180294514
Epoch: 120, Loss: 0.5189108849
Epoch: 121, Loss: 0.5174272656
Epoch: 122, Loss: 0.5165969133
Epoch: 123, Loss: 0.5188811421
Epoch: 124, Loss: 0.5174524784
Epoch: 125, Loss: 0.5172457099
Epoch: 126, Loss: 0.5183414817
Epoch: 127, Loss: 0.5184354782
Epoch: 128, Loss: 0.5163426399
Epoch: 129, Loss: 0.5180757046
Epoch: 130, Loss: 0.5172019005
Epoch: 131, Loss: 0.5165915489
Epoch: 132, Loss: 0.5166369677
Epoch: 133, Loss: 0.5190864205
Epoch: 134, Loss: 0.5170110464
Epoch: 135, Loss: 0.5174441338
Epoch: 136, Loss: 0.5156145096
Epoch: 137, Loss: 0.5155889392
Epoch: 138, Loss: 0.5154566765
Epoch: 139, Loss: 0.5162369609
Epoch: 140, Loss: 0.5168289542
Epoch: 141, Loss: 0.5162491798
Epoch: 142, Loss: 0.5189942718
Epoch: 143, Loss: 0.5154251456
Epoch: 144, Loss: 0.5183690190
Epoch: 145, Loss: 0.5163923502
Epoch: 146, Loss: 0.5190693736
Epoch: 147, Loss: 0.5158427358
Epoch: 148, Loss: 0.5175243616
Epoch: 149, Loss: 0.5163682699
Epoch: 150, Loss: 0.5165211558
100%|██████████| 3641/3641 [04:00<00:00, 15.14it/s]
              precision    recall  f1-score   support

           0       0.78      0.84      0.81      2420
           1       0.78      0.91      0.84      1198
           2       0.83      0.87      0.85      5436
           3       0.58      0.84      0.69      1061
           4       0.93      0.81      0.87     38476
           5       0.83      0.85      0.84      4278
           6       0.68      0.83      0.75      1174
           7       0.52      0.84      0.64       309
           8       0.92      0.85      0.88     20454
           9       0.76      0.85      0.80      3969
          10       0.87      0.88      0.87      2540
          11       0.88      0.87      0.87      2336
          12       0.89      0.84      0.86     11612
          13       0.85      0.87      0.86      3897
          14       0.85      0.85      0.85      2409
          15       0.79      0.87      0.83      2358
          16       0.59      0.79      0.68      1174
          17       0.88      0.86      0.87      4787
          18       0.84      0.91      0.87      1400
          19       0.76      0.79      0.78      2999
          20       0.83      0.88      0.86      4476
          21       0.88      0.89      0.89      6083
          22       0.75      0.86      0.80      2643
          23       0.64      0.83      0.73      1383
          24       0.69      0.81      0.74      2033
          25       0.88      0.90      0.89      1626
          26       0.65      0.82      0.72       506
          27       0.26      0.79      0.39       417
          28       0.73      0.85      0.79       835
          29       0.81      0.84      0.82      6596
          30       0.86      0.87      0.86      2194
          31       0.85      0.81      0.83     19516
          32       0.85      0.86      0.85      5071
          33       0.66      0.88      0.75       462
          34       0.70      0.83      0.76      2889
          35       0.87      0.84      0.85      3668
          36       0.75      0.87      0.80      2430
          37       0.86      0.83      0.85      8419
          38       0.72      0.87      0.79       790
          39       0.84      0.84      0.84      2877
          40       0.81      0.87      0.84       854
          41       0.70      0.80      0.75      2016
          42       0.88      0.84      0.86     16701
          43       0.77      0.85      0.81      2775
          44       0.76      0.89      0.82       755
          45       0.79      0.83      0.81      3497
          46       0.90      0.87      0.89     10146
          47       0.41      0.77      0.53       528
          48       0.41      0.79      0.54       616
          49       0.90      0.90      0.90      5205
          50       0.50      0.80      0.62       706

    accuracy                           0.84    233000
   macro avg       0.76      0.85      0.79    233000
weighted avg       0.85      0.84      0.84    233000

196056 233000
Train: 0.0000, Val: 0.0000, Test: 0.8414
100%|██████████| 36435/36435 [40:13<00:00, 15.10it/s] 
              precision    recall  f1-score   support

           0       0.78      0.84      0.81     24599
           1       0.81      0.90      0.85     12763
           2       0.83      0.87      0.85     54937
           3       0.52      0.84      0.64     10564
           4       0.93      0.72      0.81    384023
           5       0.84      0.85      0.84     42691
           6       0.68      0.85      0.75     12323
           7       0.31      0.84      0.45      3198
           8       0.92      0.84      0.88    203174
           9       0.76      0.83      0.80     40449
          10       0.84      0.88      0.86     25740
          11       0.86      0.87      0.87     23468
          12       0.89      0.82      0.85    116315
          13       0.86      0.85      0.85     39880
          14       0.85      0.85      0.85     24718
          15       0.79      0.87      0.83     22708
          16       0.47      0.80      0.59     10779
          17       0.85      0.87      0.86     47653
          18       0.84      0.91      0.87     14011
          19       0.75      0.80      0.77     28544
          20       0.84      0.87      0.85     44598
          21       0.87      0.89      0.88     62038
          22       0.76      0.86      0.81     26538
          23       0.61      0.83      0.70     14525
          24       0.68      0.80      0.73     20448
          25       0.86      0.91      0.88     16207
          26       0.62      0.85      0.72      5214
          27       0.16      0.81      0.27      4210
          28       0.70      0.85      0.77      8097
          29       0.82      0.82      0.82     65243
          30       0.84      0.86      0.85     22450
          31       0.84      0.79      0.81    195249
          32       0.84      0.86      0.85     50025
          33       0.60      0.87      0.71      4631
          34       0.70      0.83      0.76     28526
          35       0.86      0.84      0.85     36369
          36       0.74      0.86      0.79     23656
          37       0.87      0.81      0.84     85544
          38       0.66      0.87      0.75      7659
          39       0.84      0.84      0.84     29080
          40       0.71      0.87      0.78      8619
          41       0.68      0.81      0.74     19718
          42       0.91      0.83      0.87    165939
          43       0.78      0.84      0.81     27732
          44       0.75      0.88      0.81      7864
          45       0.79      0.82      0.81     35347
          46       0.91      0.87      0.89    102599
          47       0.25      0.76      0.38      5200
          48       0.18      0.80      0.29      6418
          49       0.90      0.89      0.89     52617
          50       0.21      0.81      0.34      6890

    accuracy                           0.82   2331787
   macro avg       0.73      0.84      0.76   2331787
weighted avg       0.85      0.82      0.83   2331787

1910012 2331787
Total labeled: 0.8191, Val: 0.0000, Test: 0.0000
Epoch: 151, Loss: 0.5145443678
Epoch: 152, Loss: 0.5167819858
Epoch: 153, Loss: 0.5169807673
Epoch: 154, Loss: 0.5159394145
Epoch: 155, Loss: 0.5167579055
Epoch: 156, Loss: 0.5169602633
Epoch: 157, Loss: 0.5164284706
Epoch: 158, Loss: 0.5172768235
Epoch: 159, Loss: 0.5159298778
Epoch: 160, Loss: 0.5172096491
Epoch: 161, Loss: 0.5155305862
Epoch: 162, Loss: 0.5150442123
Epoch: 163, Loss: 0.5153494477
Epoch: 164, Loss: 0.5160905719
Epoch: 165, Loss: 0.5146448016
Epoch: 166, Loss: 0.5164577365
Epoch: 167, Loss: 0.5152654648
Epoch: 168, Loss: 0.5165075064
Epoch: 169, Loss: 0.5142033100
Epoch: 170, Loss: 0.5164932013
Epoch: 171, Loss: 0.5152031779
Epoch: 172, Loss: 0.5165073872
Epoch: 173, Loss: 0.5167096257
Epoch: 174, Loss: 0.5152160525
Epoch: 175, Loss: 0.5155330300
Epoch: 176, Loss: 0.5146057010
Epoch: 177, Loss: 0.5172801614
Epoch: 178, Loss: 0.5175735950
Epoch: 179, Loss: 0.5170803666
Epoch: 180, Loss: 0.5165463686
Epoch: 181, Loss: 0.5166438222
Epoch: 182, Loss: 0.5165911317
Epoch: 183, Loss: 0.5166678429
Epoch: 184, Loss: 0.5161551833
Epoch: 185, Loss: 0.5146019459
Epoch: 186, Loss: 0.5168526769
Epoch: 187, Loss: 0.5155638456
Epoch: 188, Loss: 0.5162745714
Epoch: 189, Loss: 0.5170536637
Epoch: 190, Loss: 0.5141713619
Epoch: 191, Loss: 0.5154232383
Epoch: 192, Loss: 0.5151286721
Epoch: 193, Loss: 0.5159873962
Epoch: 194, Loss: 0.5145738721
Epoch: 195, Loss: 0.5140284896
Epoch: 196, Loss: 0.5158002973
Epoch: 197, Loss: 0.5151277781
Epoch: 198, Loss: 0.5167977810
Epoch: 199, Loss: 0.5152481794
Epoch: 200, Loss: 0.5157397389
100%|██████████| 3641/3641 [04:02<00:00, 15.00it/s]
              precision    recall  f1-score   support

           0       0.79      0.84      0.81      2420
           1       0.86      0.91      0.88      1198
           2       0.85      0.87      0.86      5436
           3       0.60      0.83      0.70      1061
           4       0.92      0.82      0.87     38476
           5       0.84      0.85      0.85      4278
           6       0.65      0.84      0.73      1174
           7       0.53      0.85      0.65       309
           8       0.91      0.86      0.88     20454
           9       0.80      0.83      0.82      3969
          10       0.86      0.88      0.87      2540
          11       0.85      0.88      0.86      2336
          12       0.88      0.83      0.86     11612
          13       0.84      0.87      0.86      3897
          14       0.85      0.85      0.85      2409
          15       0.79      0.88      0.83      2358
          16       0.55      0.80      0.65      1174
          17       0.88      0.86      0.87      4787
          18       0.83      0.91      0.87      1400
          19       0.77      0.79      0.78      2999
          20       0.85      0.87      0.86      4476
          21       0.88      0.89      0.89      6083
          22       0.75      0.86      0.80      2643
          23       0.75      0.82      0.78      1383
          24       0.72      0.80      0.76      2033
          25       0.88      0.90      0.89      1626
          26       0.53      0.83      0.65       506
          27       0.21      0.81      0.34       417
          28       0.70      0.86      0.77       835
          29       0.83      0.83      0.83      6596
          30       0.86      0.87      0.86      2194
          31       0.86      0.80      0.83     19516
          32       0.86      0.86      0.86      5071
          33       0.62      0.88      0.73       462
          34       0.70      0.83      0.76      2889
          35       0.88      0.83      0.86      3668
          36       0.76      0.86      0.81      2430
          37       0.86      0.84      0.85      8419
          38       0.72      0.86      0.79       790
          39       0.83      0.84      0.84      2877
          40       0.82      0.87      0.84       854
          41       0.71      0.80      0.75      2016
          42       0.88      0.84      0.86     16701
          43       0.79      0.85      0.82      2775
          44       0.73      0.89      0.80       755
          45       0.80      0.82      0.81      3497
          46       0.91      0.87      0.89     10146
          47       0.35      0.78      0.48       528
          48       0.46      0.77      0.58       616
          49       0.88      0.90      0.89      5205
          50       0.52      0.81      0.64       706

    accuracy                           0.84    233000
   macro avg       0.76      0.85      0.79    233000
weighted avg       0.85      0.84      0.85    233000

196299 233000
Train: 0.0000, Val: 0.0000, Test: 0.8425
100%|██████████| 36435/36435 [40:27<00:00, 15.01it/s] 
              precision    recall  f1-score   support

           0       0.77      0.84      0.81     24599
           1       0.86      0.90      0.88     12763
           2       0.85      0.86      0.86     54937
           3       0.55      0.83      0.66     10564
           4       0.92      0.73      0.82    384023
           5       0.85      0.84      0.85     42691
           6       0.64      0.86      0.73     12323
           7       0.26      0.85      0.39      3198
           8       0.91      0.84      0.88    203174
           9       0.80      0.82      0.81     40449
          10       0.84      0.88      0.86     25740
          11       0.83      0.88      0.85     23468
          12       0.89      0.82      0.85    116315
          13       0.84      0.86      0.85     39880
          14       0.83      0.85      0.84     24718
          15       0.79      0.88      0.83     22708
          16       0.43      0.81      0.56     10779
          17       0.87      0.86      0.87     47653
          18       0.83      0.91      0.87     14011
          19       0.77      0.79      0.78     28544
          20       0.85      0.87      0.86     44598
          21       0.88      0.89      0.88     62038
          22       0.76      0.86      0.81     26538
          23       0.66      0.82      0.73     14525
          24       0.73      0.79      0.76     20448
          25       0.86      0.91      0.89     16207
          26       0.51      0.86      0.64      5214
          27       0.11      0.83      0.19      4210
          28       0.68      0.86      0.76      8097
          29       0.83      0.82      0.82     65243
          30       0.85      0.86      0.85     22450
          31       0.86      0.78      0.82    195249
          32       0.85      0.86      0.85     50025
          33       0.48      0.89      0.63      4631
          34       0.69      0.82      0.75     28526
          35       0.88      0.83      0.85     36369
          36       0.74      0.86      0.80     23656
          37       0.86      0.82      0.84     85544
          38       0.67      0.86      0.75      7659
          39       0.84      0.84      0.84     29080
          40       0.72      0.87      0.79      8619
          41       0.69      0.81      0.75     19718
          42       0.91      0.83      0.87    165939
          43       0.78      0.84      0.81     27732
          44       0.74      0.89      0.81      7864
          45       0.81      0.82      0.81     35347
          46       0.92      0.86      0.89    102599
          47       0.27      0.78      0.41      5200
          48       0.23      0.79      0.36      6418
          49       0.88      0.90      0.89     52617
          50       0.34      0.80      0.48      6890

    accuracy                           0.82   2331787
   macro avg       0.73      0.84      0.77   2331787
weighted avg       0.85      0.82      0.83   2331787

1916206 2331787
Total labeled: 0.8218, Val: 0.0000, Test: 0.0000
Epoch: 201, Loss: 0.5146167278
Epoch: 202, Loss: 0.5171760321
Epoch: 203, Loss: 0.5169166923
Epoch: 204, Loss: 0.5170466900
Epoch: 205, Loss: 0.5154335499
Epoch: 206, Loss: 0.5148112774
Epoch: 207, Loss: 0.5162200928
Epoch: 208, Loss: 0.5159605742
Epoch: 209, Loss: 0.5148227215
Epoch: 210, Loss: 0.5151841044
Epoch: 211, Loss: 0.5137793422
Epoch: 212, Loss: 0.5157904625
Epoch: 213, Loss: 0.5151885748
Epoch: 214, Loss: 0.5156546235
Epoch: 215, Loss: 0.5166169405
Epoch: 216, Loss: 0.5166496634
Epoch: 217, Loss: 0.5152548552
Epoch: 218, Loss: 0.5162410736
Epoch: 219, Loss: 0.5175753832
Epoch: 220, Loss: 0.5166404247
Epoch: 221, Loss: 0.5152187347
Epoch: 222, Loss: 0.5150156021
Epoch: 223, Loss: 0.5161308646
Epoch: 224, Loss: 0.5157070160
Epoch: 225, Loss: 0.5166067481
Epoch: 226, Loss: 0.5160319209
Epoch: 227, Loss: 0.5148647428
Epoch: 228, Loss: 0.5158705711
Epoch: 229, Loss: 0.5140204430
Epoch: 230, Loss: 0.5144476891
Epoch: 231, Loss: 0.5154676437
Epoch: 232, Loss: 0.5139566064
Epoch: 233, Loss: 0.5160589218
Epoch: 234, Loss: 0.5152494311
Epoch: 235, Loss: 0.5142907500
Epoch: 236, Loss: 0.5162261128
Epoch: 237, Loss: 0.5165666342
Epoch: 238, Loss: 0.5143687129
Epoch: 239, Loss: 0.5148119926
Epoch: 240, Loss: 0.5163250566
Epoch: 241, Loss: 0.5147840381
Epoch: 242, Loss: 0.5142921209
Epoch: 243, Loss: 0.5158572197
Epoch: 244, Loss: 0.5182220340
Epoch: 245, Loss: 0.5158647895
Epoch: 246, Loss: 0.5171005726
Epoch: 247, Loss: 0.5150481462
Epoch: 248, Loss: 0.5156784654
Epoch: 249, Loss: 0.5152094364
Epoch: 250, Loss: 0.5160032511
100%|██████████| 3641/3641 [04:04<00:00, 14.89it/s]
              precision    recall  f1-score   support

           0       0.82      0.83      0.82      2420
           1       0.83      0.91      0.87      1198
           2       0.86      0.87      0.86      5436
           3       0.57      0.84      0.68      1061
           4       0.92      0.82      0.87     38476
           5       0.84      0.85      0.84      4278
           6       0.63      0.84      0.72      1174
           7       0.57      0.84      0.68       309
           8       0.92      0.86      0.89     20454
           9       0.78      0.84      0.81      3969
          10       0.86      0.88      0.87      2540
          11       0.87      0.87      0.87      2336
          12       0.89      0.83      0.86     11612
          13       0.83      0.87      0.85      3897
          14       0.85      0.85      0.85      2409
          15       0.80      0.88      0.83      2358
          16       0.61      0.80      0.69      1174
          17       0.89      0.86      0.87      4787
          18       0.83      0.91      0.87      1400
          19       0.77      0.78      0.78      2999
          20       0.85      0.87      0.86      4476
          21       0.88      0.89      0.89      6083
          22       0.76      0.86      0.81      2643
          23       0.76      0.83      0.79      1383
          24       0.72      0.80      0.76      2033
          25       0.89      0.90      0.89      1626
          26       0.56      0.82      0.67       506
          27       0.25      0.79      0.38       417
          28       0.73      0.86      0.79       835
          29       0.81      0.83      0.82      6596
          30       0.87      0.87      0.87      2194
          31       0.85      0.80      0.83     19516
          32       0.84      0.86      0.85      5071
          33       0.64      0.88      0.74       462
          34       0.70      0.83      0.76      2889
          35       0.85      0.84      0.85      3668
          36       0.76      0.87      0.81      2430
          37       0.87      0.83      0.85      8419
          38       0.73      0.86      0.79       790
          39       0.83      0.84      0.84      2877
          40       0.84      0.87      0.85       854
          41       0.70      0.80      0.74      2016
          42       0.89      0.84      0.86     16701
          43       0.77      0.85      0.81      2775
          44       0.76      0.89      0.82       755
          45       0.77      0.83      0.80      3497
          46       0.91      0.87      0.89     10146
          47       0.32      0.80      0.45       528
          48       0.44      0.79      0.56       616
          49       0.89      0.90      0.90      5205
          50       0.59      0.79      0.67       706

    accuracy                           0.84    233000
   macro avg       0.76      0.85      0.80    233000
weighted avg       0.85      0.84      0.85    233000

196530 233000
Train: 0.0000, Val: 0.0000, Test: 0.8435
100%|██████████| 36435/36435 [40:35<00:00, 14.96it/s] 
              precision    recall  f1-score   support

           0       0.80      0.84      0.82     24599
           1       0.80      0.90      0.85     12763
           2       0.85      0.86      0.86     54937
           3       0.55      0.83      0.66     10564
           4       0.92      0.73      0.82    384023
           5       0.84      0.85      0.84     42691
           6       0.64      0.86      0.73     12323
           7       0.26      0.85      0.40      3198
           8       0.91      0.84      0.88    203174
           9       0.78      0.83      0.80     40449
          10       0.82      0.88      0.85     25740
          11       0.85      0.88      0.87     23468
          12       0.89      0.82      0.85    116315
          13       0.83      0.86      0.85     39880
          14       0.85      0.85      0.85     24718
          15       0.79      0.87      0.83     22708
          16       0.49      0.80      0.61     10779
          17       0.87      0.86      0.87     47653
          18       0.83      0.91      0.87     14011
          19       0.77      0.79      0.78     28544
          20       0.86      0.87      0.86     44598
          21       0.87      0.89      0.88     62038
          22       0.76      0.86      0.80     26538
          23       0.66      0.82      0.73     14525
          24       0.71      0.80      0.75     20448
          25       0.86      0.91      0.88     16207
          26       0.54      0.86      0.66      5214
          27       0.11      0.81      0.20      4210
          28       0.70      0.86      0.77      8097
          29       0.82      0.82      0.82     65243
          30       0.84      0.86      0.85     22450
          31       0.85      0.79      0.82    195249
          32       0.83      0.86      0.85     50025
          33       0.53      0.89      0.66      4631
          34       0.70      0.83      0.76     28526
          35       0.84      0.84      0.84     36369
          36       0.73      0.86      0.79     23656
          37       0.88      0.81      0.84     85544
          38       0.68      0.87      0.76      7659
          39       0.84      0.84      0.84     29080
          40       0.78      0.87      0.82      8619
          41       0.67      0.81      0.73     19718
          42       0.91      0.83      0.87    165939
          43       0.76      0.84      0.80     27732
          44       0.77      0.89      0.83      7864
          45       0.78      0.83      0.80     35347
          46       0.91      0.86      0.89    102599
          47       0.20      0.79      0.31      5200
          48       0.25      0.79      0.37      6418
          49       0.89      0.89      0.89     52617
          50       0.40      0.77      0.53      6890

    accuracy                           0.82   2331787
   macro avg       0.73      0.84      0.77   2331787
weighted avg       0.85      0.82      0.83   2331787

1916180 2331787
Total labeled: 0.8218, Val: 0.0000, Test: 0.0000
Epoch: 251, Loss: 0.5149734616
Epoch: 252, Loss: 0.5143035054
Epoch: 253, Loss: 0.5182666779
Epoch: 254, Loss: 0.5134926438
Epoch: 255, Loss: 0.5142841935
Epoch: 256, Loss: 0.5153850317
Epoch: 257, Loss: 0.5154293776
Epoch: 258, Loss: 0.5150125623
Epoch: 259, Loss: 0.5143814087
Epoch: 260, Loss: 0.5148046017
Epoch: 261, Loss: 0.5137858391
Epoch: 262, Loss: 0.5155185461
Epoch: 263, Loss: 0.5152089596
Epoch: 264, Loss: 0.5164832473
Epoch: 265, Loss: 0.5155402422
Epoch: 266, Loss: 0.5147659779
Epoch: 267, Loss: 0.5136099458
Epoch: 268, Loss: 0.5147916675
Epoch: 269, Loss: 0.5154477954
Epoch: 270, Loss: 0.5140696764
Epoch: 271, Loss: 0.5154426098
Epoch: 272, Loss: 0.5134025812
Epoch: 273, Loss: 0.5152261257
Epoch: 274, Loss: 0.5141021609
Epoch: 275, Loss: 0.5152626634
Epoch: 276, Loss: 0.5155585408
Epoch: 277, Loss: 0.5164098740
Epoch: 278, Loss: 0.5156184435
Epoch: 279, Loss: 0.5159537196
Epoch: 280, Loss: 0.5126683712
Epoch: 281, Loss: 0.5149729252
Epoch: 282, Loss: 0.5152777433
Epoch: 283, Loss: 0.5148712397
Epoch: 284, Loss: 0.5131345987
Epoch: 285, Loss: 0.5143576860
Epoch: 286, Loss: 0.5124659538
Epoch: 287, Loss: 0.5149623752
Epoch: 288, Loss: 0.5141898990
Epoch: 289, Loss: 0.5145294070
Epoch: 290, Loss: 0.5153671503
Epoch: 291, Loss: 0.5140234828
Epoch: 292, Loss: 0.5141275525
Epoch: 293, Loss: 0.5148149729
Epoch: 294, Loss: 0.5139606595
Epoch: 295, Loss: 0.5143625736
Epoch: 296, Loss: 0.5156269073
Epoch: 297, Loss: 0.5165347457
Epoch: 298, Loss: 0.5120791197
Epoch: 299, Loss: 0.5146551728
Epoch: 300, Loss: 0.5155581236
100%|██████████| 3641/3641 [05:30<00:00, 11.00it/s]
              precision    recall  f1-score   support

           0       0.79      0.84      0.81      2420
           1       0.85      0.91      0.88      1198
           2       0.87      0.86      0.87      5436
           3       0.46      0.84      0.60      1061
           4       0.92      0.82      0.87     38476
           5       0.85      0.83      0.84      4278
           6       0.66      0.84      0.74      1174
           7       0.37      0.86      0.52       309
           8       0.91      0.86      0.88     20454
           9       0.77      0.84      0.80      3969
          10       0.88      0.88      0.88      2540
          11       0.86      0.87      0.87      2336
          12       0.87      0.84      0.85     11612
          13       0.84      0.87      0.85      3897
          14       0.83      0.85      0.84      2409
          15       0.81      0.87      0.84      2358
          16       0.57      0.80      0.67      1174
          17       0.90      0.86      0.88      4787
          18       0.83      0.91      0.87      1400
          19       0.77      0.78      0.78      2999
          20       0.84      0.87      0.86      4476
          21       0.88      0.89      0.89      6083
          22       0.75      0.86      0.80      2643
          23       0.72      0.82      0.77      1383
          24       0.70      0.81      0.75      2033
          25       0.88      0.90      0.89      1626
          26       0.62      0.82      0.71       506
          27       0.26      0.78      0.39       417
          28       0.69      0.85      0.76       835
          29       0.82      0.83      0.83      6596
          30       0.86      0.87      0.86      2194
          31       0.89      0.78      0.83     19516
          32       0.85      0.86      0.85      5071
          33       0.62      0.88      0.73       462
          34       0.68      0.84      0.75      2889
          35       0.86      0.84      0.85      3668
          36       0.75      0.87      0.80      2430
          37       0.86      0.83      0.85      8419
          38       0.73      0.87      0.79       790
          39       0.81      0.85      0.83      2877
          40       0.82      0.87      0.84       854
          41       0.69      0.80      0.74      2016
          42       0.90      0.84      0.87     16701
          43       0.77      0.85      0.81      2775
          44       0.74      0.89      0.81       755
          45       0.77      0.83      0.80      3497
          46       0.90      0.88      0.89     10146
          47       0.35      0.79      0.49       528
          48       0.46      0.78      0.58       616
          49       0.89      0.90      0.90      5205
          50       0.51      0.82      0.63       706

    accuracy                           0.84    233000
   macro avg       0.75      0.85      0.79    233000
weighted avg       0.85      0.84      0.85    233000

196063 233000
Train: 0.0000, Val: 0.0000, Test: 0.8415
100%|██████████| 36435/36435 [46:23<00:00, 13.09it/s]  
              precision    recall  f1-score   support

           0       0.77      0.84      0.81     24599
           1       0.81      0.90      0.85     12763
           2       0.86      0.86      0.86     54937
           3       0.46      0.84      0.60     10564
           4       0.92      0.74      0.82    384023
           5       0.86      0.83      0.84     42691
           6       0.66      0.85      0.74     12323
           7       0.15      0.87      0.26      3198
           8       0.91      0.84      0.87    203174
           9       0.76      0.83      0.79     40449
          10       0.83      0.88      0.86     25740
          11       0.84      0.88      0.86     23468
          12       0.86      0.82      0.84    116315
          13       0.83      0.86      0.85     39880
          14       0.81      0.85      0.83     24718
          15       0.81      0.87      0.84     22708
          16       0.46      0.79      0.58     10779
          17       0.90      0.86      0.88     47653
          18       0.83      0.91      0.87     14011
          19       0.77      0.79      0.78     28544
          20       0.85      0.87      0.86     44598
          21       0.87      0.89      0.88     62038
          22       0.75      0.86      0.80     26538
          23       0.70      0.83      0.76     14525
          24       0.69      0.80      0.74     20448
          25       0.85      0.91      0.88     16207
          26       0.60      0.85      0.70      5214
          27       0.18      0.81      0.29      4210
          28       0.66      0.86      0.75      8097
          29       0.83      0.82      0.82     65243
          30       0.83      0.86      0.85     22450
          31       0.88      0.77      0.82    195249
          32       0.84      0.86      0.85     50025
          33       0.51      0.89      0.65      4631
          34       0.68      0.83      0.74     28526
          35       0.87      0.83      0.85     36369
          36       0.71      0.86      0.77     23656
          37       0.87      0.81      0.84     85544
          38       0.65      0.87      0.74      7659
          39       0.81      0.85      0.83     29080
          40       0.73      0.87      0.79      8619
          41       0.67      0.81      0.73     19718
          42       0.92      0.83      0.87    165939
          43       0.77      0.84      0.80     27732
          44       0.74      0.88      0.81      7864
          45       0.77      0.82      0.80     35347
          46       0.90      0.87      0.88    102599
          47       0.29      0.78      0.42      5200
          48       0.24      0.79      0.37      6418
          49       0.88      0.89      0.89     52617
          50       0.35      0.80      0.49      6890

    accuracy                           0.82   2331787
   macro avg       0.73      0.84      0.76   2331787
weighted avg       0.85      0.82      0.83   2331787

1915539 2331787
Total labeled: 0.8215, Val: 0.0000, Test: 0.0000