1. Generate feature vectors from anchored seeds

~/code/c++/crawler_v2/latestCode/state_labeler/BFS_from_anchored_pages/run.sh > 20171219.log
~/code/c++/crawler_v2/latestCode/state_labeler/BFS_from_anchored_pages/run_2lists.sh > 20171222_2lists.log
memory: 8% ~= 120*.08 ~= 10G memory
time: 20:40 starts 23:46 ends ~= 3 hours

Output directory is /public_page_graph/Adv_BFS/BFS_output/
Total page count: 15,115,446
Other page count: 12,685,090 (pages with country labels other than U.S.)
U.S. pages count:  2,430,356 (contains U.S. pages whose city belongs to only one state in the U.S.,
                              U.S. pages whose city belongs to many states in the U.S. are thrown away.)


2. Clean all-zero feature vectors which are not reached by any seeds(anchors)

rm -f /data1/yclin123/public_page_graph/BFS_output/Guam.csv
rm -f /data1/yclin123/public_page_graph/BFS_output/Puerto Rico.csv

~/code/python2/state_labeler/python clean_data.py > 20171219_cl.log

Output directory is /public_page_graph/Adv_BFS/BFS_output_cleaned/
Total page count: 6,851,911
Other page count: 5,842,776 (pages with country labels other than U.S.)
U.S. pages count: 1,009,135 (only 1,009,135 pages are reachable from at least one anchor, all the rest U.S. pages
                             are not reachable from any anchor)

3. Select min2 feature vectors from each state and others
~/code/python2/state_labeler/python select_fv_min2.py > 20171221_sl_min2.log

    a. Calulate the minimum non-zero value (smallest) of the 102 dimension distance vector to 51 anchors for each page.
    b. For other pages, only keep smallest >= 5 the arbitrary threshold for other pages, throw away the rest pages.
    c. For U.S. pages, only keep smallest <= 3 the arbitrary threshold for U.S. pages, throw away the rest pages.

Output directory is /public_page_graph/Adv_BFS/BFS_output__min2_selected/
       Jason didn't modify page_id files in this directory.

Total page count:   641,407
Other page count:   100,000 (only keep 100,000 pages out of 3,107,168 pages, throw the rest. Pages with country labels other than U.S.)
U.S. pages count:   541,407 (contains U.S. pages whose city belongs to only one state in the U.S.,
                              U.S. pages whose city belongs to many states in the U.S. are thrown away.)


4. select N feature vectors from each state and others (N <= common minimum number, Nevada:824, N=800)
~/code/python2/state_labeler/python select_fv.py > 20171221_sl.log

Nevada has the minimum 824 pages. Jason set N = 800, means select_fv.py will just select first 800 pages from each class.

Output directory is /public_page_graph/Adv_BFS/BFS_output_min2_selected_with_id_first800/
Jerry: Copy the directory to /home/jerry/Documents/jason_code/python code/state_labeler/adv/features_adv_min2_selected800/

Total page count: 40,800
Other page count:    800 (pages with country labels other than U.S.)
U.S. pages count: 40,000 (50 states * 800 = 40,000
                            Contains U.S. pages whose city belongs to only one state in the U.S.,
                            U.S. pages whose city belongs to many states in the U.S. are thrown away.)


5. upload feature vector files into docker ./labeller/features/
6. replace others.csv by the first N feature vectors instead

7. run ML classifier
./labeller/feature_ml-master.ipynb

Jason splits the 40,800 pages, into 80% training set and 20% testing set.

The classifiers are scikit learn built in functions:
    clf1 = GaussianNB()
    clf2 = AdaBoostClassifier()
    clf3 = tree.DecisionTreeClassifier()
    clf4 = RandomForestClassifier() 


anchor distance + neighbor weights:
neighbor weights is the working part
ran classifier on anchor distance only and neighbor weights only

(40800, 204)
classifier: RandomForestClassifier()
classifier
              precision    recall  f1-score   support

           0       0.93      0.87      0.90       181
           1       0.97      0.94      0.95       154
           2       0.87      0.88      0.88       156
           3       0.93      0.88      0.90       174
           4       0.61      0.86      0.71       151
           5       0.86      0.93      0.90       158
           6       0.88      0.77      0.82       151
           7       0.97      0.91      0.94       177
           8       0.80      0.89      0.84       161
           9       0.80      0.80      0.80       147
          10       0.92      0.96      0.94       161
          11       0.91      0.92      0.92       172
          12       0.74      0.81      0.78       160
          13       0.89      0.94      0.91       161
          14       0.92      0.91      0.92       154
          15       0.89      0.88      0.88       162
          16       0.90      0.76      0.82       161
          17       0.85      0.91      0.88       148
          18       0.97      0.91      0.94       169
          19       0.86      0.82      0.84       185
          20       0.86      0.86      0.86       177
          21       0.90      0.91      0.91       162
          22       0.84      0.84      0.84       154
          23       0.92      0.87      0.89       167
          24       0.85      0.80      0.82       168
          25       0.95      0.93      0.94       132
          26       0.93      0.85      0.89       163
          27       0.89      0.72      0.80       159
          28       0.90      0.85      0.87       163
          29       0.80      0.82      0.81       147
          30       0.94      0.96      0.95       152
          31       0.62      0.71      0.66       160
          32       0.87      0.91      0.89       166
          33       0.92      0.92      0.92       145
          34       0.86      0.79      0.82       154
          35       0.92      0.89      0.91       169
          36       0.94      0.91      0.92       149
          37       0.80      0.85      0.83       165
          38       0.90      0.88      0.89       157
          39       0.91      0.89      0.90       175
          40       0.94      0.92      0.93       166
          41       0.91      0.85      0.88       151
          42       0.76      0.87      0.81       157
          43       0.82      0.88      0.85       154
          44       0.92      0.91      0.91       159
          45       0.81      0.89      0.85       143
          46       0.85      0.85      0.85       155
          47       0.93      0.80      0.86       161
          48       0.93      0.85      0.88       177
          49       0.94      0.82      0.87       137
          50       0.71      0.94      0.81       173

    accuracy                           0.87      8160
   macro avg       0.87      0.87      0.87      8160
weighted avg       0.87      0.87      0.87      8160

step 3:
 precision    recall  f1-score   support

           0       0.81      0.89      0.85      4657
           1       0.87      0.96      0.91      3486
           2       0.90      0.88      0.89     15550
           3       0.62      0.92      0.74      1742
           4       0.90      0.82      0.86     77593
           5       0.89      0.88      0.89     13600
           6       0.63      0.86      0.73      2433
           7       0.71      0.96      0.82       966
           8       0.95      0.85      0.90     50737
           9       0.74      0.84      0.79      7135
          10       0.89      0.94      0.91      8373
          11       0.90      0.93      0.91      7773
          12       0.84      0.82      0.83     22386
          13       0.90      0.91      0.90     12600
          14       0.91      0.89      0.90      6645
          15       0.87      0.90      0.89      6213
          16       0.65      0.86      0.74      2001
          17       0.92      0.93      0.93     13476
          18       0.88      0.92      0.90      3626
          19       0.69      0.80      0.74      5201
          20       0.89      0.86      0.87     10219
          21       0.92      0.91      0.92     17137
          22       0.82      0.85      0.83      5895
          23       0.83      0.90      0.86      3682
          24       0.66      0.82      0.73      3292
          25       0.92      0.96      0.94      5601
          26       0.71      0.93      0.81      1160
          27       0.27      0.93      0.41       824
          28       0.66      0.91      0.77      1955
          29       0.84      0.85      0.85     16010
          30       0.90      0.94      0.92      7491
          31       0.79      0.78      0.78     39231
          32       0.90      0.87      0.89     13269
          33       0.74      0.97      0.84      1199
          34       0.76      0.82      0.79      5224
          35       0.90      0.91      0.91      9579
          36       0.81      0.89      0.85      6117
          37       0.91      0.85      0.88     20861
          38       0.63      0.92      0.75      1560
          39       0.90      0.88      0.89      7546
          40       0.87      0.95      0.91      2711
          41       0.73      0.84      0.78      4046
          42       0.90      0.83      0.87     27016
          43       0.79      0.90      0.84      7702
          44       0.82      0.93      0.87      2666
          45       0.81      0.82      0.81      7181
          46       0.93      0.88      0.90     27932
          47       0.58      0.90      0.71      1289
          48       0.91      0.91      0.91     14799
          49       0.70      0.90      0.78      2020
          50       0.94      0.96      0.95    100000

    accuracy                           0.88    641407
   macro avg       0.81      0.89      0.84    641407
weighted avg       0.88      0.88      0.88    641407

step 2:
U.S. pages count: 1,009,135 
            precision    recall  f1-score   support

           0       0.78      0.86      0.82      9019
           1       0.84      0.93      0.88      5815
           2       0.85      0.84      0.84     25350
           3       0.57      0.85      0.68      3209
           4       0.89      0.81      0.85    176819
           5       0.85      0.83      0.84     19806
           6       0.57      0.81      0.67      4540
           7       0.63      0.89      0.73      1359
           8       0.93      0.80      0.86     86547
           9       0.72      0.80      0.76     14380
          10       0.85      0.90      0.88     11967
          11       0.86      0.88      0.87     10760
          12       0.83      0.84      0.83     54917
          13       0.86      0.85      0.86     17950
          14       0.89      0.84      0.86     10106
          15       0.84      0.85      0.84      9661
          16       0.59      0.79      0.67      3567
          17       0.90      0.90      0.90     20615
          18       0.85      0.89      0.87      5845
          19       0.67      0.78      0.72     11167
          20       0.88      0.82      0.85     18490
          21       0.90      0.86      0.88     26258
          22       0.77      0.81      0.79     10700
          23       0.79      0.84      0.81      5154
          24       0.64      0.79      0.71      7282
          25       0.87      0.94      0.90      8088
          26       0.64      0.85      0.73      1694
          27       0.21      0.81      0.33      1536
          28       0.63      0.86      0.72      3435
          29       0.81      0.79      0.80     26262
          30       0.86      0.89      0.87     10179
          31       0.81      0.77      0.79     92543
          32       0.87      0.83      0.85     20895
          33       0.68      0.91      0.78      1618
          34       0.71      0.78      0.74      9732
          35       0.87      0.87      0.87     15875
          36       0.77      0.83      0.80      9701
          37       0.89      0.80      0.84     35231
          38       0.57      0.88      0.69      2949
          39       0.87      0.82      0.84     11206
          40       0.83      0.91      0.87      3825
          41       0.65      0.79      0.71      7205
          42       0.91      0.84      0.87     65949
          43       0.72      0.86      0.78     13130
          44       0.75      0.88      0.81      3712
          45       0.79      0.79      0.79     14387
          46       0.91      0.85      0.88     51140
          47       0.53      0.83      0.65      2273
          48       0.88      0.87      0.87     22348
          49       0.61      0.81      0.70      2939
          50       0.00      0.00      0.00         0

    accuracy                           0.82   1009135
   macro avg       0.75      0.83      0.78   1009135
weighted avg       0.85      0.82      0.84   1009135

avg / total       0.83      0.80      0.81   1009135

step 1:
 precision    recall  f1-score   support

           0       0.02      0.95      0.04     25740
           1       0.83      0.43      0.56     13317
           2       0.85      0.40      0.54     57075
           3       0.60      0.28      0.39     11172
           4       0.90      0.36      0.52    409149
           5       0.85      0.39      0.54     44211
           6       0.59      0.31      0.41     12823
           7       0.60      0.47      0.52      3321
           8       0.93      0.36      0.51    210077
           9       0.73      0.31      0.43     40905
          10       0.85      0.41      0.56     27453
          11       0.86      0.42      0.57     23801
          12       0.83      0.40      0.54    121531
          13       0.86      0.39      0.54     41323
          14       0.88      0.36      0.51     25726
          15       0.84      0.38      0.52     22961
          16       0.61      0.29      0.39     11433
          17       0.90      0.40      0.55     49844
          18       0.85      0.39      0.53     14370
          19       0.68      0.32      0.43     29364
          20       0.88      0.35      0.50     46039
          21       0.90      0.38      0.53     64087
          22       0.78      0.35      0.48     27306
          23       0.79      0.32      0.46     14939
          24       0.65      0.29      0.41     21355
          25       0.85      0.50      0.63     16647
          26       0.70      0.66      0.68      5432
          27       0.22      0.30      0.25      4462
          28       0.67      0.65      0.66      8380
          29       0.81      0.33      0.47     68857
          30       0.86      0.42      0.56     22947
          31       0.81      0.34      0.48    215337
          32       0.87      0.36      0.51     51522
          33       0.69      0.36      0.48      4737
          34       0.73      0.29      0.42     29615
          35       0.87      0.39      0.54     37691
          36       0.77      0.35      0.48     24476
          37       0.89      0.35      0.50     87589
          38       0.58      0.35      0.44      7982
          39       0.87      0.34      0.49     29750
          40       0.83      0.43      0.56      8905
          41       0.67      0.31      0.42     20656
          42       0.91      0.35      0.51    168287
          43       0.72      0.41      0.53     29224
          44       0.75      0.43      0.54      8061
          45       0.79      0.34      0.47     36672
          46       0.91      0.43      0.58    106982
          47       0.55      0.31      0.40      6790
          48       0.88      0.39      0.54     52957
          49       0.61      0.37      0.46      7076
          50       0.00      0.00      0.00         0

    accuracy                           0.37   2430356
   macro avg       0.74      0.38      0.48   2430356
weighted avg       0.84      0.37      0.51   2430356



BFS anchor distance only as features

(40800, 102)
classifier: RandomForestClassifier()
classifier
              precision    recall  f1-score   support

           0       0.75      0.66      0.70       181
           1       0.82      0.79      0.81       154
           2       0.66      0.71      0.68       156
           3       0.72      0.63      0.67       174
           4       0.31      0.34      0.32       151
           5       0.58      0.78      0.67       158
           6       0.71      0.50      0.58       151
           7       0.84      0.80      0.82       177
           8       0.42      0.58      0.49       161
           9       0.50      0.44      0.47       147
          10       0.70      0.89      0.78       161
          11       0.71      0.81      0.76       172
          12       0.49      0.34      0.40       160
          13       0.67      0.84      0.75       161
          14       0.63      0.69      0.66       154
          15       0.68      0.64      0.66       162
          16       0.71      0.49      0.58       161
          17       0.64      0.86      0.74       148
          18       0.73      0.69      0.71       169
          19       0.65      0.45      0.53       185
          20       0.66      0.62      0.64       177
          21       0.62      0.78      0.69       162
          22       0.60      0.61      0.60       154
          23       0.69      0.72      0.70       167
          24       0.60      0.31      0.41       168
          25       0.74      0.87      0.80       132
          26       0.83      0.70      0.76       163
          27       0.66      0.51      0.57       159
          28       0.72      0.62      0.66       163
          29       0.41      0.63      0.50       147
          30       0.76      0.88      0.82       152
          31       0.30      0.16      0.21       160
          32       0.57      0.66      0.61       166
          33       0.75      0.73      0.74       145
          34       0.60      0.48      0.53       154
          35       0.71      0.80      0.75       169
          36       0.56      0.63      0.59       149
          37       0.53      0.58      0.55       165
          38       0.67      0.61      0.64       157
          39       0.66      0.77      0.71       175
          40       0.84      0.82      0.83       166
          41       0.67      0.52      0.58       151
          42       0.53      0.41      0.46       157
          43       0.68      0.73      0.71       154
          44       0.69      0.77      0.73       159
          45       0.53      0.52      0.53       143
          46       0.52      0.64      0.58       155
          47       0.56      0.38      0.45       161
          48       0.59      0.64      0.61       177
          49       0.72      0.64      0.68       137
          50       0.62      0.86      0.72       173

    accuracy                           0.64      8160
   macro avg       0.64      0.64      0.63      8160
weighted avg       0.64      0.64      0.63      8160

step 3:
(641407, 102) (641407, 1)
F1 score report for min2selectd:
              precision    recall  f1-score   support

           0       0.45      0.71      0.55      4657
           1       0.56      0.84      0.67      3486
           2       0.65      0.75      0.70     15550
           3       0.24      0.73      0.37      1742
           4       0.69      0.34      0.46     77593
           5       0.62      0.74      0.67     13600
           6       0.40      0.64      0.49      2433
           7       0.19      0.91      0.31       966
           8       0.73      0.59      0.65     50737
           9       0.41      0.49      0.45      7135
          10       0.54      0.86      0.66      8373
          11       0.56      0.81      0.66      7773
          12       0.64      0.39      0.49     22386
          13       0.61      0.79      0.69     12600
          14       0.57      0.71      0.63      6645
          15       0.61      0.73      0.67      6213
          16       0.28      0.70      0.40      2001
          17       0.64      0.81      0.72     13476
          18       0.49      0.76      0.60      3626
          19       0.37      0.43      0.40      5201
          20       0.62      0.57      0.59     10219
          21       0.68      0.76      0.72     17137
          22       0.58      0.59      0.58      5895
          23       0.39      0.77      0.52      3682
          24       0.32      0.43      0.37      3292
          25       0.58      0.88      0.70      5601
          26       0.37      0.85      0.51      1160
          27       0.13      0.86      0.22       824
          28       0.41      0.72      0.53      1955
          29       0.45      0.64      0.53     16010
          30       0.63      0.85      0.72      7491
          31       0.41      0.14      0.21     39231
          32       0.62      0.63      0.62     13269
          33       0.34      0.90      0.50      1199
          34       0.37      0.52      0.43      5224
          35       0.65      0.77      0.70      9579
          36       0.37      0.64      0.47      6117
          37       0.65      0.60      0.62     20861
          38       0.31      0.78      0.44      1560
          39       0.52      0.70      0.60      7546
          40       0.60      0.85      0.71      2711
          41       0.44      0.58      0.50      4046
          42       0.73      0.39      0.51     27016
          43       0.55      0.73      0.62      7702
          44       0.35      0.84      0.49      2666
          45       0.45      0.49      0.47      7181
          46       0.75      0.69      0.72     27932
          47       0.21      0.70      0.32      1289
          48       0.63      0.74      0.68     14799
          49       0.38      0.77      0.51      2020
          50       0.90      0.87      0.88    100000

    accuracy                           0.62    641407
   macro avg       0.50      0.69      0.55    641407
weighted avg       0.65      0.62      0.61    641407

step 2:
U.S. pages count: 1,009,135 
             precision    recall  f1-score   support

           0       0.35      0.52      0.42      9019
           1       0.46      0.71      0.56      5815
           2       0.54      0.60      0.57     25350
           3       0.20      0.45      0.28      3209
           4       0.65      0.24      0.36    176819
           5       0.57      0.54      0.56     19806
           6       0.23      0.46      0.31      4540
           7       0.15      0.70      0.25      1359
           8       0.68      0.38      0.49     86547
           9       0.31      0.32      0.32     14380
          10       0.37      0.71      0.49     11967
          11       0.49      0.64      0.56     10760
          12       0.54      0.38      0.44     54917
          13       0.52      0.60      0.56     17950
          14       0.48      0.52      0.50     10106
          15       0.51      0.51      0.51      9661
          16       0.19      0.51      0.28      3567
          17       0.58      0.66      0.62     20615
          18       0.32      0.64      0.43      5845
          19       0.29      0.31      0.30     11167
          20       0.46      0.41      0.43     18490
          21       0.57      0.57      0.57     26258
          22       0.41      0.45      0.43     10700
          23       0.33      0.56      0.42      5154
          24       0.25      0.26      0.26      7282
          25       0.51      0.78      0.61      8088
          26       0.30      0.63      0.41      1694
          27       0.09      0.51      0.16      1536
          28       0.28      0.52      0.36      3435
          29       0.35      0.46      0.40     26262
          30       0.48      0.72      0.58     10179
          31       0.40      0.11      0.17     92543
          32       0.58      0.45      0.51     20895
          33       0.24      0.71      0.36      1618
          34       0.27      0.32      0.29      9732
          35       0.46      0.67      0.54     15875
          36       0.32      0.44      0.37      9701
          37       0.61      0.39      0.47     35231
          38       0.23      0.63      0.33      2949
          39       0.48      0.51      0.49     11206
          40       0.41      0.76      0.53      3825
          41       0.34      0.35      0.34      7205
          42       0.66      0.31      0.42     65949
          43       0.42      0.58      0.49     13130
          44       0.29      0.65      0.40      3712
          45       0.35      0.35      0.35     14387
          46       0.71      0.52      0.60     51140
          47       0.13      0.49      0.21      2273
          48       0.57      0.53      0.55     22348
          49       0.31      0.57      0.40      2939
          50       0.00      0.00      0.00         0

    accuracy                           0.40   1009135
   macro avg       0.40      0.50      0.42   1009135
weighted avg       0.53      0.40      0.43   1009135

step 1:
precision    recall  f1-score   support

           0       0.01      0.83      0.03     25740
           1       0.45      0.31      0.37     13317
           2       0.54      0.27      0.36     57075
           3       0.19      0.13      0.16     11172
           4       0.65      0.12      0.20    409149
           5       0.57      0.24      0.34     44211
           6       0.23      0.16      0.19     12823
           7       0.16      0.29      0.21      3321
           8       0.69      0.16      0.26    210077
           9       0.31      0.11      0.17     40905
          10       0.38      0.31      0.34     27453
          11       0.49      0.29      0.36     23801
          12       0.55      0.17      0.26    121531
          13       0.52      0.26      0.35     41323
          14       0.49      0.20      0.29     25726
          15       0.52      0.21      0.30     22961
          16       0.20      0.16      0.18     11433
          17       0.59      0.27      0.37     49844
          18       0.34      0.26      0.29     14370
          19       0.29      0.12      0.17     29364
          20       0.49      0.16      0.24     46039
          21       0.57      0.23      0.33     64087
          22       0.40      0.18      0.25     27306
          23       0.34      0.19      0.25     14939
          24       0.24      0.09      0.13     21355
          25       0.50      0.38      0.43     16647
          26       0.32      0.19      0.24      5432
          27       0.10      0.18      0.13      4462
          28       0.34      0.21      0.26      8380
          29       0.35      0.18      0.23     68857
          30       0.49      0.32      0.39     22947
          31       0.40      0.05      0.09    215337
          32       0.57      0.19      0.28     51522
          33       0.24      0.25      0.24      4737
          34       0.28      0.11      0.15     29615
          35       0.47      0.28      0.35     37691
          36       0.31      0.17      0.22     24476
          37       0.61      0.16      0.25     87589
          38       0.23      0.24      0.23      7982
          39       0.49      0.19      0.28     29750
          40       0.43      0.32      0.37      8905
          41       0.32      0.13      0.18     20656
          42       0.67      0.12      0.20    168287
          43       0.41      0.26      0.32     29224
          44       0.29      0.30      0.29      8061
          45       0.36      0.14      0.20     36672
          46       0.71      0.24      0.36    106982
          47       0.13      0.16      0.15      6790
          48       0.57      0.22      0.32     52957
          49       0.31      0.24      0.27      7076
          50       0.00      0.00      0.00         0

    accuracy                           0.18   2430356
   macro avg       0.39      0.22      0.25   2430356
weighted avg       0.53      0.18      0.24   2430356


only neighbor weights as features:

(40800, 102)
classifier: RandomForestClassifier()
classifier
              precision    recall  f1-score   support

           0       0.92      0.86      0.89       181
           1       0.96      0.93      0.94       154
           2       0.86      0.87      0.86       156
           3       0.93      0.87      0.90       174
           4       0.61      0.83      0.70       151
           5       0.86      0.91      0.88       158
           6       0.85      0.80      0.82       151
           7       0.96      0.91      0.93       177
           8       0.80      0.88      0.84       161
           9       0.83      0.82      0.82       147
          10       0.95      0.96      0.95       161
          11       0.92      0.92      0.92       172
          12       0.72      0.80      0.76       160
          13       0.88      0.93      0.90       161
          14       0.93      0.91      0.92       154
          15       0.86      0.88      0.87       162
          16       0.88      0.75      0.81       161
          17       0.84      0.88      0.86       148
          18       0.94      0.92      0.93       169
          19       0.86      0.81      0.83       185
          20       0.86      0.85      0.86       177
          21       0.90      0.91      0.91       162
          22       0.86      0.81      0.83       154
          23       0.92      0.86      0.89       167
          24       0.83      0.80      0.82       168
          25       0.93      0.94      0.94       132
          26       0.93      0.86      0.89       163
          27       0.91      0.71      0.80       159
          28       0.91      0.84      0.87       163
          29       0.81      0.82      0.81       147
          30       0.95      0.96      0.95       152
          31       0.62      0.70      0.66       160
          32       0.86      0.93      0.90       166
          33       0.92      0.91      0.91       145
          34       0.84      0.77      0.81       154
          35       0.91      0.88      0.89       169
          36       0.93      0.91      0.92       149
          37       0.81      0.85      0.83       165
          38       0.90      0.88      0.89       157
          39       0.92      0.88      0.90       175
          40       0.94      0.91      0.93       166
          41       0.82      0.86      0.84       151
          42       0.75      0.86      0.80       157
          43       0.80      0.84      0.82       154
          44       0.94      0.92      0.93       159
          45       0.87      0.88      0.87       143
          46       0.86      0.85      0.85       155
          47       0.91      0.80      0.85       161
          48       0.89      0.83      0.86       177
          49       0.90      0.82      0.86       137
          50       0.71      0.91      0.80       173

    accuracy                           0.86      8160
   macro avg       0.87      0.86      0.86      8160
weighted avg       0.87      0.86      0.86      8160

step 3:
(641407, 102) (641407, 1)
F1 score report for min2selectd:
              precision    recall  f1-score   support

           0       0.76      0.89      0.82      4657
           1       0.85      0.95      0.90      3486
           2       0.89      0.88      0.89     15550
           3       0.68      0.90      0.77      1742
           4       0.90      0.80      0.85     77593
           5       0.88      0.88      0.88     13600
           6       0.63      0.86      0.73      2433
           7       0.65      0.94      0.77       966
           8       0.95      0.85      0.90     50737
           9       0.72      0.83      0.77      7135
          10       0.88      0.94      0.91      8373
          11       0.89      0.92      0.90      7773
          12       0.83      0.82      0.82     22386
          13       0.89      0.90      0.89     12600
          14       0.90      0.89      0.90      6645
          15       0.84      0.90      0.87      6213
          16       0.54      0.86      0.66      2001
          17       0.92      0.93      0.92     13476
          18       0.86      0.93      0.89      3626
          19       0.65      0.80      0.72      5201
          20       0.89      0.85      0.87     10219
          21       0.92      0.91      0.92     17137
          22       0.80      0.85      0.82      5895
          23       0.80      0.90      0.85      3682
          24       0.63      0.81      0.71      3292
          25       0.89      0.96      0.92      5601
          26       0.73      0.92      0.81      1160
          27       0.24      0.88      0.38       824
          28       0.69      0.90      0.78      1955
          29       0.82      0.85      0.84     16010
          30       0.90      0.93      0.91      7491
          31       0.81      0.75      0.78     39231
          32       0.90      0.87      0.88     13269
          33       0.77      0.95      0.85      1199
          34       0.73      0.81      0.77      5224
          35       0.88      0.91      0.89      9579
          36       0.81      0.89      0.85      6117
          37       0.91      0.86      0.88     20861
          38       0.63      0.91      0.74      1560
          39       0.88      0.88      0.88      7546
          40       0.84      0.95      0.89      2711
          41       0.68      0.85      0.76      4046
          42       0.89      0.82      0.86     27016
          43       0.77      0.90      0.83      7702
          44       0.80      0.93      0.86      2666
          45       0.81      0.82      0.82      7181
          46       0.93      0.87      0.90     27932
          47       0.51      0.88      0.64      1289
          48       0.89      0.91      0.90     14799
          49       0.62      0.89      0.73      2020
          50       0.95      0.96      0.95    100000

    accuracy                           0.87    641407
   macro avg       0.79      0.88      0.83    641407
weighted avg       0.88      0.87      0.87    641407

step 2:
precision    recall  f1-score   support

           0       0.72      0.85      0.78      9019
           1       0.82      0.92      0.87      5815
           2       0.83      0.84      0.84     25350
           3       0.60      0.83      0.70      3209
           4       0.91      0.79      0.85    176819
           5       0.83      0.83      0.83     19806
           6       0.57      0.82      0.67      4540
           7       0.56      0.87      0.68      1359
           8       0.94      0.80      0.86     86547
           9       0.69      0.80      0.74     14380
          10       0.84      0.90      0.87     11967
          11       0.82      0.88      0.85     10760
          12       0.83      0.83      0.83     54917
          13       0.85      0.85      0.85     17950
          14       0.88      0.84      0.86     10106
          15       0.80      0.85      0.82      9661
          16       0.46      0.79      0.58      3567
          17       0.89      0.89      0.89     20615
          18       0.82      0.90      0.86      5845
          19       0.64      0.79      0.71     11167
          20       0.88      0.81      0.85     18490
          21       0.89      0.87      0.88     26258
          22       0.75      0.81      0.78     10700
          23       0.75      0.84      0.79      5154
          24       0.60      0.79      0.69      7282
          25       0.84      0.94      0.88      8088
          26       0.64      0.84      0.72      1694
          27       0.19      0.78      0.31      1536
          28       0.62      0.85      0.72      3435
          29       0.77      0.79      0.78     26262
          30       0.85      0.88      0.87     10179
          31       0.83      0.74      0.78     92543
          32       0.87      0.82      0.85     20895
          33       0.68      0.90      0.78      1618
          34       0.68      0.77      0.72      9732
          35       0.84      0.87      0.86     15875
          36       0.75      0.83      0.79      9701
          37       0.89      0.81      0.85     35231
          38       0.57      0.87      0.69      2949
          39       0.83      0.82      0.83     11206
          40       0.79      0.91      0.85      3825
          41       0.62      0.79      0.69      7205
          42       0.90      0.83      0.86     65949
          43       0.71      0.86      0.78     13130
          44       0.71      0.88      0.79      3712
          45       0.79      0.79      0.79     14387
          46       0.91      0.85      0.88     51140
          47       0.44      0.82      0.57      2273
          48       0.86      0.87      0.87     22348
          49       0.54      0.82      0.65      2939
          50       0.00      0.00      0.00         0

    accuracy                           0.82   1009135
   macro avg       0.73      0.82      0.77   1009135
weighted avg       0.85      0.82      0.83   1009135

step 1:
precision    recall  f1-score   support

           0       0.06      0.84      0.12     25740
           1       0.83      0.82      0.83     13317
           2       0.81      0.73      0.77     57075
           3       0.68      0.60      0.64     11172
           4       0.89      0.43      0.58    409149
           5       0.84      0.71      0.77     44211
           6       0.64      0.65      0.65     12823
           7       0.63      0.69      0.66      3321
           8       0.91      0.70      0.79    210077
           9       0.74      0.65      0.69     40905
          10       0.87      0.78      0.82     27453
          11       0.80      0.76      0.78     23801
          12       0.82      0.72      0.77    121531
          13       0.84      0.71      0.77     41323
          14       0.85      0.69      0.76     25726
          15       0.78      0.73      0.75     22961
          16       0.58      0.57      0.57     11433
          17       0.90      0.75      0.82     49844
          18       0.81      0.75      0.78     14370
          19       0.69      0.62      0.65     29364
          20       0.87      0.67      0.76     46039
          21       0.90      0.72      0.80     64087
          22       0.76      0.68      0.72     27306
          23       0.78      0.68      0.73     14939
          24       0.63      0.65      0.64     21355
          25       0.82      0.84      0.83     16647
          26       0.68      0.66      0.67      5432
          27       0.02      0.58      0.04      4462
          28       0.61      0.68      0.64      8380
          29       0.79      0.66      0.72     68857
          30       0.86      0.79      0.82     22947
          31       0.82      0.63      0.71    215337
          32       0.86      0.69      0.77     51522
          33       0.05      0.74      0.10      4737
          34       0.69      0.61      0.65     29615
          35       0.82      0.72      0.77     37691
          36       0.78      0.70      0.74     24476
          37       0.83      0.70      0.76     87589
          38       0.59      0.68      0.63      7982
          39       0.81      0.69      0.75     29750
          40       0.79      0.78      0.79      8905
          41       0.66      0.63      0.65     20656
          42       0.90      0.70      0.78    168287
          43       0.71      0.72      0.71     29224
          44       0.72      0.72      0.72      8061
          45       0.77      0.65      0.71     36672
          46       0.88      0.77      0.82    106982
          47       0.51      0.59      0.55      6790
          48       0.86      0.76      0.81     52957
          49       0.55      0.66      0.60      7076
          50       0.00      0.00      0.00         0

    accuracy                           0.65   2430356
   macro avg       0.71      0.68      0.67   2430356
weighted avg       0.83      0.65      0.72   2430356