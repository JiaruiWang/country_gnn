100%|██████████| 1278/1278 [42:28<00:00,  1.99s/it] 
Epoch: 001, Loss: 2.8092899323
100%|██████████| 73/73 [03:44<00:00,  3.08s/it]
Output exceeds the size limit. Open the full output data in a text editor
              precision    recall  f1-score   support

           0       0.87      0.81      0.83      6173
           1       0.92      0.86      0.89      1807
           2       0.87      0.86      0.87     10911
           3       0.87      0.83      0.85      3761
           4       0.72      0.81      0.77     87293
           5       0.84      0.86      0.85     12822
           6       0.84      0.81      0.83      5222
           7       0.69      0.40      0.51       456
           8       0.85      0.87      0.86     40633
           9       0.85      0.82      0.83     13881
          10       0.92      0.80      0.86      2767
          11       0.86      0.79      0.83      3248
          12       0.80      0.73      0.76     25901
          13       0.87      0.82      0.85      9600
          14       0.88      0.83      0.85      6141
          15       0.86      0.84      0.85      6720
          16       0.81      0.70      0.75      3749
          17       0.89      0.83      0.86      6567
          18       0.87      0.87      0.87      4064
          19       0.83      0.79      0.81      8592
          20       0.86      0.86      0.86     14286
          21       0.88      0.88      0.88     17693
          22       0.81      0.86      0.83      9394
...
   macro avg       0.84      0.78      0.80    590000
weighted avg       0.81      0.80      0.80    590000

473741 590000
100%|██████████| 7/7 [00:18<00:00,  2.60s/it]
Output exceeds the size limit. Open the full output data in a text editor
              precision    recall  f1-score   support

           0       0.96      0.80      0.87      1000
           1       0.99      0.86      0.92      1000
           2       0.91      0.86      0.88      1000
           3       0.98      0.81      0.89      1000
           4       0.19      0.82      0.31      1000
           5       0.79      0.86      0.82      1000
           6       0.95      0.81      0.88      1000
           7       0.99      0.41      0.58      1000
           8       0.65      0.87      0.75      1000
           9       0.87      0.81      0.84      1000
          10       0.99      0.80      0.89      1000
          11       0.94      0.80      0.87      1000
          12       0.66      0.75      0.70      1000
          13       0.93      0.82      0.87      1000
          14       0.91      0.83      0.87      1000
          15       0.90      0.81      0.86      1000
          16       0.96      0.72      0.82      1000
          17       0.95      0.84      0.89      1000
          18       0.95      0.88      0.91      1000
          19       0.75      0.80      0.78      1000
          20       0.79      0.86      0.82      1000
          21       0.85      0.89      0.87      1000
          22       0.81      0.85      0.83      1000
...
weighted avg       0.87      0.78      0.80     51000

39929 51000
Train: 0.0000, Val: 0.8030, Test: 0.7829
100%|██████████| 1278/1278 [35:59<00:00,  1.69s/it] 
Epoch: 002, Loss: 0.9745581150
100%|██████████| 73/73 [02:43<00:00,  2.24s/it]
Output exceeds the size limit. Open the full output data in a text editor
              precision    recall  f1-score   support

           0       0.87      0.82      0.84      6173
           1       0.92      0.87      0.89      1807
           2       0.88      0.87      0.87     10911
           3       0.87      0.83      0.85      3761
           4       0.73      0.82      0.77     87293
           5       0.84      0.86      0.85     12822
           6       0.85      0.82      0.83      5222
           7       0.69      0.45      0.55       456
           8       0.86      0.87      0.86     40633
           9       0.85      0.83      0.84     13881
          10       0.92      0.81      0.86      2767
          11       0.86      0.80      0.83      3248
          12       0.81      0.74      0.77     25901
          13       0.88      0.82      0.85      9600
          14       0.89      0.83      0.86      6141
          15       0.86      0.85      0.85      6720
          16       0.82      0.72      0.77      3749
          17       0.89      0.83      0.86      6567
          18       0.88      0.87      0.88      4064
          19       0.84      0.79      0.82      8592
          20       0.87      0.86      0.87     14286
          21       0.89      0.88      0.88     17693
          22       0.81      0.86      0.84      9394
...
   macro avg       0.84      0.79      0.81    590000
weighted avg       0.81      0.81      0.81    590000

478046 590000
100%|██████████| 7/7 [00:13<00:00,  1.92s/it]
Output exceeds the size limit. Open the full output data in a text editor
              precision    recall  f1-score   support

           0       0.96      0.81      0.88      1000
           1       0.99      0.88      0.93      1000
           2       0.92      0.86      0.89      1000
           3       0.98      0.83      0.90      1000
           4       0.20      0.82      0.32      1000
           5       0.79      0.86      0.83      1000
           6       0.95      0.82      0.88      1000
           7       0.99      0.45      0.62      1000
           8       0.68      0.87      0.76      1000
           9       0.87      0.82      0.85      1000
          10       0.99      0.81      0.89      1000
          11       0.95      0.81      0.87      1000
          12       0.66      0.76      0.71      1000
          13       0.93      0.82      0.87      1000
          14       0.92      0.84      0.88      1000
          15       0.90      0.83      0.86      1000
          16       0.96      0.73      0.83      1000
          17       0.95      0.84      0.89      1000
          18       0.95      0.87      0.91      1000
          19       0.77      0.81      0.79      1000
          20       0.81      0.86      0.84      1000
          21       0.85      0.89      0.87      1000
          22       0.83      0.84      0.84      1000
...
weighted avg       0.87      0.79      0.81     51000

40369 51000
Train: 0.0000, Val: 0.8102, Test: 0.7915
100%|██████████| 1278/1278 [36:25<00:00,  1.71s/it]
Epoch: 003, Loss: 0.9297653437
100%|██████████| 73/73 [03:38<00:00,  3.00s/it]
Output exceeds the size limit. Open the full output data in a text editor
              precision    recall  f1-score   support

           0       0.87      0.82      0.85      6173
           1       0.92      0.87      0.89      1807
           2       0.88      0.87      0.87     10911
           3       0.87      0.84      0.85      3761
           4       0.73      0.83      0.78     87293
           5       0.84      0.87      0.85     12822
           6       0.85      0.82      0.84      5222
           7       0.70      0.46      0.56       456
           8       0.86      0.87      0.87     40633
           9       0.86      0.83      0.84     13881
          10       0.93      0.81      0.86      2767
          11       0.86      0.80      0.83      3248
          12       0.81      0.75      0.78     25901
          13       0.89      0.82      0.85      9600
          14       0.89      0.83      0.86      6141
          15       0.86      0.85      0.85      6720
          16       0.82      0.74      0.78      3749
          17       0.90      0.84      0.86      6567
          18       0.88      0.88      0.88      4064
          19       0.84      0.80      0.82      8592
          20       0.87      0.87      0.87     14286
          21       0.89      0.88      0.88     17693
          22       0.82      0.86      0.84      9394
...
   macro avg       0.84      0.79      0.81    590000
weighted avg       0.82      0.81      0.81    590000

479528 590000
100%|██████████| 7/7 [00:17<00:00,  2.50s/it]
Output exceeds the size limit. Open the full output data in a text editor
              precision    recall  f1-score   support

           0       0.96      0.81      0.88      1000
           1       0.99      0.87      0.93      1000
           2       0.92      0.86      0.89      1000
           3       0.98      0.83      0.90      1000
           4       0.20      0.83      0.32      1000
           5       0.80      0.86      0.83      1000
           6       0.96      0.82      0.88      1000
           7       0.99      0.47      0.64      1000
           8       0.69      0.87      0.77      1000
           9       0.87      0.82      0.85      1000
          10       0.99      0.81      0.89      1000
          11       0.95      0.81      0.87      1000
          12       0.66      0.76      0.71      1000
          13       0.93      0.82      0.87      1000
          14       0.92      0.84      0.88      1000
          15       0.91      0.83      0.87      1000
          16       0.96      0.75      0.84      1000
          17       0.95      0.84      0.89      1000
          18       0.95      0.88      0.91      1000
          19       0.77      0.82      0.79      1000
          20       0.82      0.86      0.84      1000
          21       0.86      0.89      0.87      1000
          22       0.84      0.85      0.84      1000
...
weighted avg       0.88      0.79      0.81     51000

40498 51000
Train: 0.0000, Val: 0.8128, Test: 0.7941
100%|██████████| 1278/1278 [34:24<00:00,  1.62s/it] 
Epoch: 004, Loss: 0.9127740860
100%|██████████| 73/73 [02:52<00:00,  2.36s/it]
Output exceeds the size limit. Open the full output data in a text editor
              precision    recall  f1-score   support

           0       0.87      0.82      0.85      6173
           1       0.92      0.87      0.90      1807
           2       0.88      0.87      0.88     10911
           3       0.88      0.84      0.86      3761
           4       0.73      0.83      0.78     87293
           5       0.84      0.87      0.85     12822
           6       0.86      0.82      0.84      5222
           7       0.69      0.48      0.56       456
           8       0.86      0.87      0.87     40633
           9       0.86      0.83      0.85     13881
          10       0.93      0.81      0.86      2767
          11       0.86      0.80      0.83      3248
          12       0.82      0.74      0.78     25901
          13       0.89      0.82      0.86      9600
          14       0.89      0.83      0.86      6141
          15       0.86      0.85      0.86      6720
          16       0.83      0.74      0.78      3749
          17       0.90      0.84      0.87      6567
          18       0.88      0.87      0.88      4064
          19       0.84      0.81      0.82      8592
          20       0.87      0.87      0.87     14286
          21       0.89      0.88      0.88     17693
          22       0.82      0.86      0.84      9394
...
   macro avg       0.84      0.80      0.82    590000
weighted avg       0.82      0.81      0.82    590000

480789 590000
100%|██████████| 7/7 [00:13<00:00,  1.96s/it]
Output exceeds the size limit. Open the full output data in a text editor
              precision    recall  f1-score   support

           0       0.96      0.81      0.88      1000
           1       0.99      0.88      0.93      1000
           2       0.92      0.86      0.89      1000
           3       0.98      0.82      0.89      1000
           4       0.20      0.82      0.32      1000
           5       0.80      0.86      0.83      1000
           6       0.96      0.82      0.88      1000
           7       0.99      0.49      0.66      1000
           8       0.69      0.88      0.77      1000
           9       0.87      0.83      0.85      1000
          10       0.99      0.81      0.89      1000
          11       0.95      0.81      0.88      1000
          12       0.68      0.75      0.71      1000
          13       0.93      0.82      0.87      1000
          14       0.92      0.84      0.88      1000
          15       0.91      0.83      0.87      1000
          16       0.97      0.75      0.84      1000
          17       0.95      0.84      0.89      1000
          18       0.96      0.87      0.91      1000
          19       0.77      0.82      0.79      1000
          20       0.81      0.86      0.84      1000
          21       0.86      0.89      0.87      1000
          22       0.84      0.85      0.84      1000
...
weighted avg       0.88      0.80      0.81     51000

40560 51000
Train: 0.0000, Val: 0.8149, Test: 0.7953
100%|██████████| 1278/1278 [31:43<00:00,  1.49s/it]
Epoch: 005, Loss: 0.9159703255
100%|██████████| 73/73 [03:00<00:00,  2.48s/it]
Output exceeds the size limit. Open the full output data in a text editor
              precision    recall  f1-score   support

           0       0.87      0.82      0.85      6173
           1       0.91      0.88      0.89      1807
           2       0.88      0.87      0.88     10911
           3       0.88      0.84      0.86      3761
           4       0.73      0.83      0.78     87293
           5       0.84      0.87      0.86     12822
           6       0.85      0.83      0.84      5222
           7       0.64      0.50      0.56       456
           8       0.86      0.87      0.87     40633
           9       0.85      0.84      0.85     13881
          10       0.93      0.81      0.86      2767
          11       0.86      0.81      0.83      3248
          12       0.82      0.74      0.78     25901
          13       0.89      0.83      0.86      9600
          14       0.89      0.83      0.86      6141
          15       0.86      0.85      0.86      6720
          16       0.83      0.74      0.78      3749
          17       0.90      0.84      0.87      6567
          18       0.88      0.88      0.88      4064
          19       0.83      0.81      0.82      8592
          20       0.87      0.87      0.87     14286
          21       0.90      0.88      0.89     17693
          22       0.82      0.86      0.84      9394
...
   macro avg       0.84      0.80      0.82    590000
weighted avg       0.82      0.82      0.82    590000

481241 590000
100%|██████████| 7/7 [00:14<00:00,  2.05s/it]
Output exceeds the size limit. Open the full output data in a text editor
              precision    recall  f1-score   support

           0       0.96      0.82      0.88      1000
           1       0.99      0.88      0.93      1000
           2       0.92      0.86      0.89      1000
           3       0.98      0.83      0.90      1000
           4       0.20      0.83      0.32      1000
           5       0.80      0.86      0.83      1000
           6       0.95      0.82      0.88      1000
           7       0.99      0.51      0.68      1000
           8       0.69      0.88      0.77      1000
           9       0.86      0.83      0.85      1000
          10       0.99      0.82      0.89      1000
          11       0.95      0.82      0.88      1000
          12       0.68      0.75      0.71      1000
          13       0.93      0.82      0.88      1000
          14       0.93      0.84      0.88      1000
          15       0.91      0.83      0.87      1000
          16       0.97      0.75      0.84      1000
          17       0.95      0.84      0.89      1000
          18       0.96      0.88      0.92      1000
          19       0.77      0.82      0.79      1000
          20       0.81      0.86      0.84      1000
          21       0.87      0.89      0.88      1000
          22       0.85      0.85      0.85      1000
...
weighted avg       0.88      0.80      0.82     51000

40707 51000
Train: 0.0000, Val: 0.8157, Test: 0.7982
100%|██████████| 1278/1278 [33:49<00:00,  1.59s/it]
Output exceeds the size limit. Open the full output data in a text editor
training set report:
              precision    recall  f1-score   support

           0       0.87      0.83      0.85     53439
           1       0.93      0.87      0.90     16447
           2       0.88      0.87      0.87     96885
           3       0.87      0.84      0.85     32976
           4       0.73      0.83      0.78    772818
           5       0.85      0.87      0.86    113161
           6       0.85      0.84      0.84     46038
           7       0.66      0.53      0.59      4482
           8       0.86      0.87      0.87    361591
           9       0.85      0.84      0.84    123727
          10       0.92      0.80      0.86     24145
          11       0.87      0.81      0.84     29483
          12       0.82      0.75      0.78    230237
          13       0.88      0.83      0.86     86203
          14       0.88      0.82      0.85     53358
          15       0.86      0.86      0.86     59206
          16       0.83      0.74      0.79     34109
          17       0.90      0.84      0.87     57493
          18       0.88      0.88      0.88     35262
          19       0.83      0.81      0.82     75970
          20       0.88      0.87      0.87    127866
          21       0.89      0.88      0.89    155763
          22       0.82      0.85      0.84     83342
          23       0.84      0.79      0.82     24373
          24       0.84      0.77      0.80     57991
          25       0.90      0.87      0.88     25446
          26       0.84      0.85      0.85     24391
          27       0.87      0.77      0.82     38125
          28       0.85      0.78      0.81     27759
          29       0.85      0.80      0.82    106258
          30       0.89      0.75      0.81     27853
          31       0.71      0.70      0.71    478117
          32       0.87      0.85      0.86    159592
          33       0.87      0.83      0.85     11092
          34       0.84      0.84      0.84    148839
          35       0.87      0.79      0.83     51169
          36       0.86      0.88      0.87     83760
          37       0.85      0.78      0.82    207143
          38       0.88      0.76      0.82     18199
          39       0.87      0.85      0.86     65653
          40       0.88      0.84      0.86     14447
          41       0.85      0.85      0.85     77814
          42       0.75      0.78      0.76    457931
          43       0.85      0.78      0.82     39936
          44       0.86      0.83      0.84     17212
          45       0.87      0.83      0.85    119227
          46       0.88      0.86      0.87    144270
          47       0.25      0.07      0.10      3898
          48       0.79      0.74      0.76     15657
          49       0.89      0.87      0.88    101028
          50       0.88      0.70      0.78     11214

    accuracy                           0.82   5232395
   macro avg       0.84      0.80      0.82   5232395
weighted avg       0.82      0.82      0.82   5232395

4265237 5232395

validation set report:
              precision    recall  f1-score   support

           0       0.87      0.82      0.85      6173
           1       0.91      0.88      0.89      1807
           2       0.88      0.87      0.88     10911
           3       0.88      0.84      0.86      3761
           4       0.73      0.83      0.78     87293
           5       0.84      0.87      0.86     12822
           6       0.85      0.83      0.84      5222
           7       0.64      0.50      0.56       456
           8       0.86      0.87      0.87     40633
           9       0.85      0.84      0.85     13881
          10       0.93      0.81      0.86      2767
          11       0.86      0.81      0.83      3248
          12       0.82      0.74      0.78     25901
          13       0.89      0.83      0.86      9600
          14       0.89      0.83      0.86      6141
          15       0.86      0.85      0.86      6720
          16       0.83      0.74      0.78      3749
          17       0.90      0.84      0.87      6567
          18       0.88      0.88      0.88      4064
          19       0.83      0.81      0.82      8592
          20       0.87      0.87      0.87     14286
          21       0.90      0.88      0.89     17693
          22       0.82      0.86      0.84      9394
          23       0.85      0.79      0.82      2736
          24       0.84      0.77      0.81      6615
          25       0.90      0.87      0.88      2930
          26       0.84      0.85      0.85      2683
          27       0.88      0.78      0.82      4349
          28       0.85      0.78      0.81      3084
          29       0.84      0.80      0.82     11897
          30       0.88      0.74      0.81      3122
          31       0.71      0.71      0.71     53712
          32       0.88      0.85      0.86     17999
          33       0.88      0.84      0.86      1243
          34       0.85      0.85      0.85     16919
          35       0.87      0.79      0.83      5777
          36       0.86      0.88      0.87      9256
          37       0.85      0.78      0.81     23333
          38       0.87      0.77      0.82      2049
          39       0.87      0.85      0.86      7256
          40       0.89      0.87      0.88      1606
          41       0.85      0.85      0.85      8862
          42       0.74      0.78      0.76     51785
          43       0.84      0.79      0.81      4552
          44       0.86      0.82      0.84      1968
          45       0.87      0.83      0.85     13318
          46       0.89      0.86      0.87     16331
          47       0.27      0.08      0.12       408
          48       0.78      0.72      0.75      1730
          49       0.89      0.88      0.88     11513
          50       0.87      0.71      0.78      1286

    accuracy                           0.82    590000
   macro avg       0.84      0.80      0.82    590000
weighted avg       0.82      0.82      0.82    590000

481241 590000

test set report:

              precision    recall  f1-score   support

           0       0.96      0.82      0.88      1000
           1       0.99      0.88      0.93      1000
           2       0.92      0.86      0.89      1000
           3       0.98      0.83      0.90      1000
           4       0.20      0.83      0.32      1000
           5       0.80      0.86      0.83      1000
           6       0.95      0.82      0.88      1000
           7       0.99      0.51      0.68      1000
           8       0.69      0.88      0.77      1000
           9       0.86      0.83      0.85      1000
          10       0.99      0.82      0.89      1000
          11       0.95      0.82      0.88      1000
          12       0.68      0.75      0.71      1000
          13       0.93      0.82      0.88      1000
          14       0.93      0.84      0.88      1000
          15       0.91      0.83      0.87      1000
          16       0.97      0.75      0.84      1000
          17       0.95      0.84      0.89      1000
          18       0.96      0.88      0.92      1000
          19       0.77      0.82      0.79      1000
          20       0.81      0.86      0.84      1000
          21       0.87      0.89      0.88      1000
          22       0.85      0.85      0.85      1000
          23       0.98      0.79      0.87      1000
          24       0.91      0.75      0.82      1000
          25       0.96      0.86      0.91      1000
          26       0.96      0.84      0.90      1000
          27       0.96      0.80      0.88      1000
          28       0.96      0.80      0.87      1000
          29       0.91      0.79      0.85      1000
          30       0.98      0.74      0.84      1000
          31       0.32      0.69      0.44      1000
          32       0.84      0.85      0.84      1000
          33       0.98      0.84      0.91      1000
          34       0.83      0.83      0.83      1000
          35       0.94      0.79      0.86      1000
          36       0.91      0.88      0.89      1000
          37       0.71      0.78      0.74      1000
          38       0.98      0.79      0.87      1000
          39       0.94      0.87      0.90      1000
          40       0.97      0.86      0.91      1000
          41       0.92      0.83      0.88      1000
          42       0.36      0.77      0.49      1000
          43       0.94      0.79      0.86      1000
          44       0.98      0.84      0.90      1000
          45       0.84      0.82      0.83      1000
          46       0.85      0.85      0.85      1000
          47       1.00      0.06      0.11      1000
          48       0.98      0.74      0.84      1000
          49       0.92      0.86      0.89      1000
          50       0.98      0.71      0.82      1000

    accuracy                           0.80     51000
   macro avg       0.88      0.80      0.82     51000
weighted avg       0.88      0.80      0.82     51000

40707 51000
Train: 0.8152, Val: 0.8157, Test: 0.7982

