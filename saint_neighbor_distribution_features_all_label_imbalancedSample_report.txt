train_loader = GraphSAINTRandomWalkSampler(data, batch_size=600000, walk_length=2,
                                           num_steps=5, sample_coverage=100,
                                           save_dir=dataset.processed_dir,
                                           num_workers=20) 
              precision    recall  f1-score   support

           0       0.86      0.86      0.86      6022
           1       0.91      0.90      0.90      1907
           2       0.86      0.90      0.88     10889
           3       0.88      0.88      0.88      3716
           4       0.82      0.83      0.82     86735
           5       0.88      0.88      0.88     12945
           6       0.88      0.86      0.87      5106
           7       0.77      0.70      0.73       501
           8       0.87      0.88      0.88     40892
           9       0.86      0.87      0.87     14042
          10       0.89      0.86      0.87      2794
          11       0.86      0.87      0.86      3324
          12       0.83      0.79      0.81     25842
          13       0.86      0.88      0.87      9501
          14       0.86      0.86      0.86      6046
          15       0.86      0.89      0.88      6564
          16       0.88      0.81      0.84      3815
          17       0.88      0.88      0.88      6388
          18       0.87      0.91      0.89      3924
          19       0.84      0.86      0.85      8586
          20       0.89      0.89      0.89     14303
          21       0.89      0.91      0.90     17810
          22       0.84      0.90      0.87      9375
          23       0.86      0.83      0.84      2720
          24       0.84      0.82      0.83      6641
          25       0.90      0.90      0.90      2935
          26       0.88      0.91      0.89      2701
          27       0.84      0.84      0.84      4272
          28       0.85      0.88      0.86      3113
          29       0.84      0.84      0.84     12048
          30       0.87      0.80      0.83      3201
          31       0.75      0.76      0.76     54092
          32       0.87      0.88      0.88     18171
          33       0.88      0.88      0.88      1280
          34       0.87      0.87      0.87     16736
          35       0.87      0.85      0.86      5783
          36       0.88      0.90      0.89      9435
          37       0.85      0.81      0.83     23263
          38       0.85      0.86      0.86      2026
          39       0.87      0.89      0.88      7384
          40       0.89      0.89      0.89      1652
          41       0.86      0.88      0.87      8702
          42       0.82      0.79      0.81     51608
          43       0.82      0.85      0.84      4531
          44       0.87      0.87      0.87      1988
          45       0.86      0.87      0.87     13469
          46       0.89      0.88      0.89     16334
          47       0.62      0.40      0.49       440
          48       0.82      0.81      0.81      1787
          49       0.89      0.91      0.90     11427
          50       0.85      0.79      0.82      1234

    accuracy                           0.84    590000
   macro avg       0.86      0.85      0.85    590000
weighted avg       0.84      0.84      0.84    590000

498298 590000


              precision    recall  f1-score   support

           0       0.94      0.89      0.91      1000
           1       0.99      0.91      0.95      1000
           2       0.93      0.89      0.91      1000
           3       0.97      0.86      0.91      1000
           4       0.33      0.82      0.47      1000
           5       0.83      0.86      0.85      1000
           6       0.96      0.86      0.91      1000
           7       0.99      0.72      0.83      1000
           8       0.70      0.87      0.78      1000
           9       0.88      0.86      0.87      1000
          10       0.99      0.85      0.92      1000
          11       0.96      0.86      0.91      1000
          12       0.71      0.78      0.75      1000
          13       0.92      0.88      0.90      1000
          14       0.93      0.87      0.90      1000
          15       0.92      0.88      0.90      1000
          16       0.97      0.80      0.88      1000
          17       0.93      0.86      0.90      1000
          18       0.96      0.92      0.94      1000
          19       0.83      0.86      0.85      1000
          20       0.84      0.88      0.86      1000
          21       0.88      0.90      0.89      1000
          22       0.84      0.90      0.87      1000
          23       0.97      0.82      0.89      1000
          24       0.92      0.83      0.87      1000
          25       0.97      0.89      0.93      1000
          26       0.97      0.92      0.94      1000
          27       0.96      0.86      0.91      1000
          28       0.96      0.86      0.91      1000
          29       0.91      0.85      0.88      1000
          30       0.97      0.81      0.89      1000
          31       0.41      0.79      0.54      1000
          32       0.82      0.88      0.85      1000
          33       0.97      0.87      0.92      1000
          34       0.85      0.86      0.86      1000
          35       0.94      0.82      0.88      1000
          36       0.92      0.91      0.91      1000
          37       0.73      0.82      0.77      1000
          38       0.98      0.84      0.91      1000
          39       0.94      0.88      0.91      1000
          40       0.97      0.89      0.93      1000
          41       0.92      0.89      0.90      1000
          42       0.49      0.79      0.61      1000
          43       0.92      0.86      0.89      1000
          44       0.98      0.89      0.93      1000
          45       0.84      0.86      0.85      1000
          46       0.84      0.88      0.86      1000
          47       1.00      0.40      0.57      1000
          48       0.97      0.78      0.87      1000
          49       0.92      0.92      0.92      1000
          50       0.98      0.79      0.87      1000

    accuracy                           0.85     51000
   macro avg       0.89      0.85      0.86     51000
weighted avg       0.89      0.85      0.86     51000

43253 51000
Train: 0.0000, Val: 0.8446, Test: 0.8481

train_loader = GraphSAINTRandomWalkSampler(data, batch_size=600000, walk_length=2,
                                           num_steps=5, sample_coverage=1000,
                                           save_dir=dataset.processed_dir,
                                           num_workers=20)

              precision    recall  f1-score   support

           0       0.86      0.84      0.85      6056
           1       0.93      0.89      0.91      1898
           2       0.87      0.88      0.88     10923
           3       0.86      0.86      0.86      3679
           4       0.79      0.82      0.81     86986
           5       0.85      0.88      0.86     12762
           6       0.85      0.86      0.86      5187
           7       0.81      0.53      0.64       514
           8       0.85      0.88      0.87     41015
           9       0.85      0.85      0.85     13886
          10       0.91      0.83      0.86      2767
          11       0.84      0.84      0.84      3258
          12       0.83      0.76      0.79     26043
          13       0.87      0.86      0.86      9619
          14       0.89      0.82      0.85      5987
          15       0.86      0.86      0.86      6718
          16       0.80      0.80      0.80      3806
          17       0.90      0.85      0.88      6537
          18       0.88      0.89      0.89      3935
          19       0.82      0.83      0.83      8492
          20       0.86      0.89      0.87     14312
          21       0.88      0.89      0.89     17549
          22       0.84      0.87      0.85      9507
          23       0.86      0.78      0.82      2718
          24       0.81      0.80      0.81      6548
          25       0.89      0.87      0.88      2832
          26       0.87      0.85      0.86      2818
          27       0.87      0.79      0.83      4326
          28       0.81      0.86      0.83      3187
          29       0.83      0.83      0.83     11997
          30       0.88      0.75      0.81      3191
          31       0.73      0.73      0.73     53849
          32       0.87      0.86      0.86     17986
          33       0.87      0.85      0.86      1224
          34       0.85      0.85      0.85     16780
          35       0.86      0.82      0.84      5696
          36       0.87      0.89      0.88      9382
          37       0.83      0.80      0.82     23508
          38       0.85      0.84      0.85      2093
          39       0.87      0.87      0.87      7276
          40       0.87      0.86      0.87      1564
          41       0.84      0.86      0.85      8857
          42       0.79      0.78      0.78     51486
          43       0.85      0.82      0.83      4620
          44       0.86      0.84      0.85      1971
          45       0.85      0.86      0.85     13491
          46       0.88      0.87      0.87     16353
          47       0.67      0.05      0.10       428
          48       0.84      0.70      0.76      1756
          49       0.88      0.90      0.89     11370
          50       0.86      0.72      0.78      1257

    accuracy                           0.83    590000
   macro avg       0.85      0.82      0.83    590000
weighted avg       0.83      0.83      0.83    590000

488488 590000

              precision    recall  f1-score   support

           0       0.94      0.84      0.89      1000
           1       0.99      0.87      0.93      1000
           2       0.91      0.88      0.89      1000
           3       0.97      0.87      0.91      1000
           4       0.26      0.83      0.39      1000
           5       0.78      0.88      0.83      1000
           6       0.94      0.87      0.90      1000
           7       0.99      0.57      0.72      1000
           8       0.67      0.87      0.76      1000
           9       0.86      0.84      0.85      1000
          10       0.99      0.82      0.90      1000
          11       0.96      0.85      0.90      1000
          12       0.71      0.77      0.74      1000
          13       0.92      0.86      0.89      1000
          14       0.93      0.82      0.87      1000
          15       0.91      0.87      0.89      1000
          16       0.95      0.79      0.86      1000
          17       0.95      0.85      0.90      1000
          18       0.96      0.90      0.93      1000
          19       0.76      0.83      0.80      1000
          20       0.81      0.90      0.85      1000
          21       0.88      0.89      0.89      1000
          22       0.83      0.86      0.85      1000
          23       0.96      0.79      0.87      1000
          24       0.89      0.82      0.86      1000
          25       0.96      0.87      0.91      1000
          26       0.97      0.85      0.91      1000
          27       0.97      0.80      0.88      1000
          28       0.94      0.88      0.91      1000
          29       0.91      0.81      0.86      1000
          30       0.97      0.76      0.85      1000
          31       0.38      0.73      0.50      1000
          32       0.83      0.85      0.84      1000
          33       0.98      0.83      0.90      1000
          34       0.81      0.85      0.83      1000
          35       0.92      0.81      0.86      1000
          36       0.93      0.91      0.92      1000
          37       0.69      0.83      0.75      1000
          38       0.98      0.81      0.89      1000
          39       0.93      0.88      0.90      1000
          40       0.97      0.86      0.91      1000
          41       0.91      0.87      0.89      1000
          42       0.43      0.78      0.56      1000
          43       0.92      0.81      0.86      1000
          44       0.98      0.86      0.92      1000
          45       0.78      0.85      0.81      1000
          46       0.85      0.88      0.86      1000
          47       1.00      0.08      0.14      1000
          48       0.98      0.70      0.82      1000
          49       0.90      0.90      0.90      1000
          50       0.98      0.71      0.83      1000

    accuracy                           0.82     51000
   macro avg       0.87      0.82      0.83     51000
weighted avg       0.87      0.82      0.83     51000

41750 51000
Train: 0.0000, Val: 0.8279, Test: 0.8186

train_loader = GraphSAINTRandomWalkSampler(data, batch_size=600000, walk_length=2,
                                           num_steps=5, sample_coverage=500,
                                           save_dir=dataset.processed_dir,
                                           num_workers=20)

              precision    recall  f1-score   support

           0       0.85      0.85      0.85      6028
           1       0.92      0.89      0.90      1899
           2       0.87      0.88      0.87     11012
           3       0.87      0.86      0.86      3748
           4       0.79      0.82      0.81     86816
           5       0.85      0.88      0.87     12738
           6       0.85      0.87      0.86      5137
           7       0.73      0.59      0.65       520
           8       0.86      0.88      0.87     40360
           9       0.85      0.85      0.85     13980
          10       0.91      0.83      0.87      2782
          11       0.85      0.83      0.84      3344
          12       0.82      0.76      0.79     26141
          13       0.86      0.86      0.86      9778
          14       0.88      0.84      0.86      6078
          15       0.85      0.86      0.85      6667
          16       0.81      0.78      0.80      3800
          17       0.90      0.85      0.87      6601
          18       0.86      0.90      0.88      3959
          19       0.82      0.84      0.83      8501
          20       0.87      0.89      0.88     14447
          21       0.88      0.89      0.88     17675
          22       0.82      0.87      0.85      9455
          23       0.83      0.80      0.82      2697
          24       0.81      0.81      0.81      6513
          25       0.89      0.87      0.88      2895
          26       0.83      0.89      0.86      2787
          27       0.84      0.81      0.82      4198
          28       0.82      0.86      0.84      3097
          29       0.83      0.83      0.83     11936
          30       0.90      0.75      0.82      3134
          31       0.75      0.73      0.74     54013
          32       0.86      0.86      0.86     18105
          33       0.87      0.83      0.85      1213
          34       0.85      0.85      0.85     16871
          35       0.85      0.82      0.83      5789
          36       0.87      0.90      0.88      9469
          37       0.82      0.80      0.81     23530
          38       0.85      0.83      0.84      2057
          39       0.87      0.86      0.87      7299
          40       0.87      0.86      0.86      1601
          41       0.85      0.85      0.85      8689
          42       0.80      0.78      0.79     51613
          43       0.83      0.82      0.82      4488
          44       0.86      0.85      0.86      1989
          45       0.83      0.87      0.85     13521
          46       0.87      0.87      0.87     16329
          47       0.56      0.09      0.16       433
          48       0.82      0.75      0.79      1691
          49       0.87      0.90      0.89     11332
          50       0.84      0.75      0.79      1245

    accuracy                           0.83    590000
   macro avg       0.84      0.82      0.83    590000
weighted avg       0.83      0.83      0.83    590000

488798 590000

              precision    recall  f1-score   support

           0       0.94      0.86      0.90      1000
           1       0.98      0.88      0.93      1000
           2       0.91      0.86      0.89      1000
           3       0.96      0.85      0.90      1000
           4       0.27      0.83      0.40      1000
           5       0.77      0.86      0.81      1000
           6       0.96      0.88      0.92      1000
           7       0.99      0.62      0.76      1000
           8       0.69      0.88      0.77      1000
           9       0.88      0.85      0.86      1000
          10       0.98      0.83      0.90      1000
          11       0.95      0.84      0.89      1000
          12       0.69      0.76      0.73      1000
          13       0.92      0.86      0.89      1000
          14       0.93      0.85      0.89      1000
          15       0.91      0.87      0.89      1000
          16       0.94      0.79      0.86      1000
          17       0.94      0.84      0.89      1000
          18       0.96      0.92      0.94      1000
          19       0.77      0.85      0.81      1000
          20       0.82      0.91      0.86      1000
          21       0.86      0.89      0.88      1000
          22       0.84      0.85      0.84      1000
          23       0.96      0.80      0.88      1000
          24       0.90      0.80      0.85      1000
          25       0.97      0.88      0.93      1000
          26       0.95      0.90      0.92      1000
          27       0.95      0.80      0.87      1000
          28       0.94      0.86      0.90      1000
          29       0.89      0.81      0.85      1000
          30       0.97      0.75      0.85      1000
          31       0.37      0.71      0.48      1000
          32       0.79      0.86      0.82      1000
          33       0.99      0.83      0.90      1000
          34       0.83      0.86      0.84      1000
          35       0.91      0.82      0.86      1000
          36       0.90      0.88      0.89      1000
          37       0.71      0.81      0.76      1000
          38       0.98      0.83      0.90      1000
          39       0.95      0.87      0.91      1000
          40       0.97      0.87      0.91      1000
          41       0.92      0.86      0.89      1000
          42       0.44      0.78      0.56      1000
          43       0.95      0.82      0.88      1000
          44       0.99      0.86      0.92      1000
          45       0.76      0.86      0.80      1000
          46       0.83      0.85      0.84      1000
          47       1.00      0.10      0.18      1000
          48       0.98      0.73      0.83      1000
          49       0.90      0.89      0.90      1000
          50       0.97      0.74      0.84      1000

    accuracy                           0.82     51000
   macro avg       0.87      0.82      0.83     51000
weighted avg       0.87      0.82      0.83     51000

41851 51000
Train: 0.0000, Val: 0.8285, Test: 0.8206

train_loader = GraphSAINTRandomWalkSampler(data, batch_size=600000, walk_length=2,
                                           num_steps=5, sample_coverage=50,
                                           save_dir=dataset.processed_dir,
                                           num_workers=20)

              precision    recall  f1-score   support

           0       0.85      0.86      0.86      6073
           1       0.91      0.89      0.90      1918
           2       0.87      0.87      0.87     10924
           3       0.84      0.86      0.85      3593
           4       0.79      0.83      0.81     87165
           5       0.85      0.88      0.87     12827
           6       0.85      0.87      0.86      5100
           7       0.75      0.62      0.67       505
           8       0.87      0.87      0.87     40769
           9       0.84      0.85      0.85     14125
          10       0.90      0.83      0.87      2739
          11       0.85      0.82      0.84      3271
          12       0.83      0.75      0.79     26054
          13       0.86      0.86      0.86      9662
          14       0.88      0.83      0.86      5995
          15       0.85      0.85      0.85      6545
          16       0.81      0.80      0.80      3859
          17       0.89      0.85      0.87      6378
          18       0.88      0.91      0.89      4034
          19       0.82      0.83      0.83      8656
          20       0.87      0.89      0.88     14399
          21       0.89      0.89      0.89     17618
          22       0.84      0.87      0.85      9425
          23       0.85      0.80      0.83      2806
          24       0.82      0.80      0.81      6559
          25       0.89      0.87      0.88      2975
          26       0.84      0.88      0.86      2597
          27       0.84      0.80      0.82      4409
          28       0.82      0.87      0.84      3158
          29       0.84      0.82      0.83     12007
          30       0.89      0.75      0.81      3286
          31       0.74      0.74      0.74     53714
          32       0.87      0.86      0.87     17980
          33       0.86      0.84      0.85      1273
          34       0.85      0.85      0.85     16848
          35       0.86      0.81      0.83      5792
          36       0.86      0.90      0.88      9509
          37       0.84      0.80      0.82     23273
          38       0.84      0.86      0.85      2033
          39       0.87      0.85      0.86      7317
          40       0.87      0.85      0.86      1648
          41       0.84      0.87      0.85      8796
          42       0.78      0.78      0.78     51474
          43       0.82      0.81      0.82      4449
          44       0.87      0.86      0.86      1977
          45       0.86      0.85      0.85     13446
          46       0.88      0.87      0.87     16290
          47       0.29      0.13      0.18       435
          48       0.80      0.75      0.77      1743
          49       0.88      0.89      0.88     11324
          50       0.86      0.73      0.79      1248

    accuracy                           0.83    590000
   macro avg       0.84      0.82      0.83    590000
weighted avg       0.83      0.83      0.83    590000

488630 590000
              precision    recall  f1-score   support

           0       0.93      0.85      0.89      1000
           1       0.99      0.89      0.94      1000
           2       0.92      0.87      0.89      1000
           3       0.95      0.86      0.90      1000
           4       0.26      0.82      0.40      1000
           5       0.77      0.85      0.81      1000
           6       0.96      0.85      0.90      1000
           7       0.99      0.60      0.75      1000
           8       0.70      0.87      0.78      1000
           9       0.88      0.86      0.87      1000
          10       0.98      0.82      0.90      1000
          11       0.96      0.84      0.89      1000
          12       0.71      0.77      0.74      1000
          13       0.92      0.85      0.88      1000
          14       0.93      0.82      0.88      1000
          15       0.90      0.86      0.88      1000
          16       0.95      0.81      0.87      1000
          17       0.94      0.85      0.90      1000
          18       0.95      0.91      0.93      1000
          19       0.75      0.81      0.78      1000
          20       0.81      0.88      0.84      1000
          21       0.86      0.90      0.88      1000
          22       0.86      0.87      0.86      1000
          23       0.96      0.80      0.87      1000
          24       0.91      0.79      0.84      1000
          25       0.96      0.87      0.91      1000
          26       0.96      0.88      0.92      1000
          27       0.95      0.79      0.86      1000
          28       0.93      0.86      0.89      1000
          29       0.88      0.83      0.85      1000
          30       0.97      0.76      0.85      1000
          31       0.39      0.76      0.52      1000
          32       0.85      0.86      0.85      1000
          33       0.98      0.83      0.90      1000
          34       0.81      0.84      0.82      1000
          35       0.91      0.80      0.85      1000
          36       0.92      0.91      0.91      1000
          37       0.70      0.82      0.76      1000
          38       0.97      0.84      0.90      1000
          39       0.95      0.86      0.90      1000
          40       0.97      0.86      0.91      1000
          41       0.92      0.87      0.89      1000
          42       0.43      0.79      0.56      1000
          43       0.92      0.83      0.87      1000
          44       0.97      0.85      0.90      1000
          45       0.80      0.87      0.83      1000
          46       0.83      0.87      0.85      1000
          47       0.95      0.14      0.24      1000
          48       0.98      0.74      0.85      1000
          49       0.92      0.91      0.91      1000
          50       0.97      0.72      0.82      1000

    accuracy                           0.82     51000
   macro avg       0.87      0.82      0.83     51000
weighted avg       0.87      0.82      0.83     51000

41875 51000
Train: 0.0000, Val: 0.8282, Test: 0.8211

train_loader = GraphSAINTRandomWalkSampler(data, batch_size=600000, walk_length=2,
                                           num_steps=5, sample_coverage=25,
                                           save_dir=dataset.processed_dir,
                                           num_workers=20)

              precision    recall  f1-score   support

           0       0.85      0.86      0.85      5972
           1       0.89      0.90      0.90      1905
           2       0.87      0.87      0.87     10980
           3       0.85      0.87      0.86      3670
           4       0.75      0.83      0.79     87066
           5       0.84      0.88      0.86     12834
           6       0.83      0.86      0.85      5091
           7       0.65      0.67      0.66       488
           8       0.85      0.87      0.86     40744
           9       0.84      0.85      0.85     13968
          10       0.90      0.83      0.86      2683
          11       0.84      0.84      0.84      3305
          12       0.81      0.74      0.78     25983
          13       0.84      0.86      0.85      9765
          14       0.87      0.84      0.85      5983
          15       0.84      0.87      0.85      6686
          16       0.78      0.82      0.80      3826
          17       0.89      0.84      0.87      6561
          18       0.85      0.91      0.88      4015
          19       0.83      0.82      0.82      8787
          20       0.85      0.88      0.87     14305
          21       0.88      0.89      0.88     17586
          22       0.81      0.87      0.84      9229
          23       0.81      0.82      0.82      2734
          24       0.82      0.78      0.80      6545
          25       0.88      0.88      0.88      2938
          26       0.83      0.85      0.84      2750
          27       0.82      0.80      0.81      4444
          28       0.80      0.86      0.82      3160
          29       0.83      0.81      0.82     11907
          30       0.88      0.76      0.81      3080
          31       0.77      0.68      0.72     54106
          32       0.86      0.85      0.86     18119
          33       0.85      0.87      0.86      1258
          34       0.84      0.85      0.85     16741
          35       0.86      0.81      0.84      5695
          36       0.83      0.89      0.86      9370
          37       0.83      0.79      0.81     23354
          38       0.84      0.85      0.85      1997
          39       0.85      0.86      0.85      7347
          40       0.87      0.85      0.86      1598
          41       0.83      0.86      0.85      8851
          42       0.79      0.76      0.77     51383
          43       0.83      0.83      0.83      4528
          44       0.83      0.85      0.84      1879
          45       0.85      0.85      0.85     13403
          46       0.87      0.86      0.87     16436
          47       0.55      0.05      0.09       471
          48       0.78      0.75      0.77      1764
          49       0.87      0.89      0.88     11458
          50       0.83      0.72      0.77      1252

    accuracy                           0.82    590000
   macro avg       0.83      0.82      0.82    590000
weighted avg       0.82      0.82      0.82    590000

482791 590000

              precision    recall  f1-score   support

           0       0.93      0.86      0.89      1000
           1       0.99      0.89      0.94      1000
           2       0.89      0.87      0.88      1000
           3       0.96      0.86      0.91      1000
           4       0.23      0.81      0.36      1000
           5       0.77      0.89      0.83      1000
           6       0.94      0.87      0.90      1000
           7       0.98      0.64      0.77      1000
           8       0.65      0.85      0.74      1000
           9       0.87      0.85      0.86      1000
          10       0.98      0.82      0.90      1000
          11       0.95      0.84      0.89      1000
          12       0.68      0.75      0.71      1000
          13       0.91      0.85      0.88      1000
          14       0.89      0.82      0.86      1000
          15       0.90      0.87      0.88      1000
          16       0.94      0.80      0.86      1000
          17       0.95      0.83      0.89      1000
          18       0.96      0.91      0.93      1000
          19       0.76      0.79      0.78      1000
          20       0.79      0.88      0.83      1000
          21       0.88      0.89      0.88      1000
          22       0.84      0.86      0.85      1000
          23       0.95      0.81      0.88      1000
          24       0.89      0.80      0.84      1000
          25       0.95      0.87      0.91      1000
          26       0.96      0.89      0.92      1000
          27       0.95      0.80      0.87      1000
          28       0.93      0.85      0.89      1000
          29       0.91      0.80      0.85      1000
          30       0.96      0.73      0.83      1000
          31       0.42      0.69      0.53      1000
          32       0.84      0.85      0.85      1000
          33       0.97      0.88      0.92      1000
          34       0.81      0.84      0.83      1000
          35       0.93      0.82      0.87      1000
          36       0.91      0.88      0.89      1000
          37       0.70      0.79      0.74      1000
          38       0.98      0.83      0.90      1000
          39       0.94      0.86      0.90      1000
          40       0.97      0.84      0.90      1000
          41       0.90      0.85      0.87      1000
          42       0.44      0.76      0.56      1000
          43       0.93      0.82      0.87      1000
          44       0.98      0.85      0.91      1000
          45       0.79      0.86      0.82      1000
          46       0.83      0.87      0.85      1000
          47       1.00      0.06      0.11      1000
          48       0.96      0.75      0.84      1000
          49       0.91      0.90      0.91      1000
          50       0.97      0.70      0.82      1000

    accuracy                           0.82     51000
   macro avg       0.87      0.82      0.83     51000
weighted avg       0.87      0.82      0.83     51000

41570 51000
Train: 0.0000, Val: 0.8183, Test: 0.8151
