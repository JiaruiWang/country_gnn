Epoch: 001, Loss: 0.0000865910
Epoch: 002, Loss: 0.0000462056
Epoch: 003, Loss: 0.0000340989
Epoch: 004, Loss: 0.0000298637
Epoch: 005, Loss: 0.0000284428
Epoch: 006, Loss: 0.0000275948
Epoch: 007, Loss: 0.0000269258
Epoch: 008, Loss: 0.0000266169
Epoch: 009, Loss: 0.0000262602
Epoch: 010, Loss: 0.0000260861
Train: 0.8321, Val: 0.8389, Test: 0.8394
Epoch: 011, Loss: 0.0000259404
Epoch: 012, Loss: 0.0000258222
Epoch: 013, Loss: 0.0000258211
Epoch: 014, Loss: 0.0000254610
Epoch: 015, Loss: 0.0000254307
Epoch: 016, Loss: 0.0000253656
Epoch: 017, Loss: 0.0000256433
Epoch: 018, Loss: 0.0000252990
Epoch: 019, Loss: 0.0000252686
Epoch: 020, Loss: 0.0000252061
Train: 0.8335, Val: 0.8394, Test: 0.8395
Epoch: 021, Loss: 0.0000252481
Epoch: 022, Loss: 0.0000250436
Epoch: 023, Loss: 0.0000250909
Epoch: 024, Loss: 0.0000250801
Epoch: 025, Loss: 0.0000250567
Epoch: 026, Loss: 0.0000251062
Epoch: 027, Loss: 0.0000251078
Epoch: 028, Loss: 0.0000249638
Epoch: 029, Loss: 0.0000249516
Epoch: 030, Loss: 0.0000249718
Train: 0.8358, Val: 0.8384, Test: 0.8387
Epoch: 031, Loss: 0.0000253408
Epoch: 032, Loss: 0.0000249079
Epoch: 033, Loss: 0.0000247969
Epoch: 034, Loss: 0.0000248104
Epoch: 035, Loss: 0.0000248260
Epoch: 036, Loss: 0.0000248366
Epoch: 037, Loss: 0.0000248836
Epoch: 038, Loss: 0.0000249440
Epoch: 039, Loss: 0.0000250050
Epoch: 040, Loss: 0.0000249018
Train: 0.8359, Val: 0.8401, Test: 0.8401

training set report:
              precision    recall  f1-score   support

           0       0.90      0.83      0.86     36357
           1       0.97      0.89      0.93     36790
           2       0.85      0.87      0.86     36572
           3       0.92      0.81      0.86     37059
           4       0.50      0.82      0.62     36900
           5       0.75      0.85      0.80     36542
           6       0.92      0.83      0.87     36569
           7       0.96      0.80      0.87     36697
           8       0.68      0.86      0.76     36268
           9       0.81      0.83      0.82     36589
          10       0.95      0.87      0.91     36283
          11       0.91      0.86      0.89     36537
          12       0.72      0.84      0.77     36697
          13       0.88      0.86      0.87     36304
          14       0.89      0.83      0.86     36530
          15       0.88      0.86      0.87     36684
          16       0.90      0.76      0.82     36622
          17       0.92      0.85      0.88     36581
          18       0.94      0.90      0.92     36662
          19       0.79      0.77      0.78     37051
          20       0.81      0.86      0.84     36663
          21       0.85      0.89      0.87     36442
          22       0.80      0.86      0.83     36202
          23       0.93      0.81      0.87     36373
          24       0.85      0.79      0.82     36514
          25       0.95      0.90      0.92     36399
          26       0.93      0.83      0.88     36861
          27       0.89      0.79      0.84     36413
          28       0.91      0.84      0.87     36856
          29       0.81      0.83      0.82     36083
          30       0.95      0.85      0.90     36489
          31       0.55      0.81      0.66     36610
          32       0.79      0.87      0.82     36644
          33       0.96      0.86      0.91     36519
          34       0.77      0.82      0.79     36568
          35       0.86      0.83      0.85     36828
          36       0.88      0.85      0.86     36388
          37       0.72      0.82      0.77     36685
          38       0.94      0.85      0.90     36470
          39       0.90      0.84      0.87     36446
          40       0.95      0.86      0.90     36416
          41       0.87      0.79      0.83     36510
          42       0.59      0.85      0.70     36419
          43       0.86      0.84      0.85     36648
          44       0.96      0.87      0.91     36823
          45       0.81      0.80      0.81     36564
          46       0.83      0.86      0.84     36493
          47       0.78      0.74      0.76     36840
          48       0.93      0.74      0.82     37277
          49       0.86      0.89      0.88     36578
          50       0.88      0.78      0.82     36472

    accuracy                           0.84   1865787
   macro avg       0.85      0.84      0.84   1865787
weighted avg       0.85      0.84      0.84   1865787

1559676 1865787

validation set report:
              precision    recall  f1-score   support

           0       0.83      0.81      0.82      2519
           1       0.89      0.90      0.90      1329
           2       0.85      0.88      0.86      5484
           3       0.75      0.83      0.79      1146
           4       0.90      0.82      0.86     38183
           5       0.78      0.84      0.81      4331
           6       0.74      0.84      0.79      1212
           7       0.58      0.84      0.68       329
           8       0.90      0.86      0.88     20358
           9       0.79      0.81      0.80      4000
          10       0.87      0.88      0.88      2617
          11       0.88      0.86      0.87      2408
          12       0.86      0.84      0.85     11608
          13       0.86      0.85      0.86      4027
          14       0.86      0.84      0.85      2467
          15       0.82      0.87      0.84      2272
          16       0.70      0.75      0.73      1111
          17       0.92      0.85      0.88      4711
          18       0.86      0.89      0.88      1422
          19       0.79      0.79      0.79      2852
          20       0.87      0.86      0.86      4533
          21       0.88      0.89      0.88      6203
          22       0.75      0.87      0.80      2656
          23       0.79      0.81      0.80      1473
          24       0.73      0.76      0.75      2082
          25       0.89      0.90      0.89      1586
          26       0.69      0.84      0.76       523
          27       0.28      0.76      0.41       418
          28       0.74      0.87      0.80       851
          29       0.82      0.83      0.82      6639
          30       0.89      0.85      0.87      2288
          31       0.82      0.80      0.81     19403
          32       0.81      0.87      0.84      5011
          33       0.79      0.87      0.83       444
          34       0.70      0.82      0.76      2892
          35       0.87      0.83      0.85      3621
          36       0.75      0.85      0.80      2318
          37       0.86      0.83      0.85      8379
          38       0.73      0.86      0.79       755
          39       0.84      0.83      0.84      2855
          40       0.87      0.85      0.86       875
          41       0.75      0.78      0.76      1966
          42       0.84      0.86      0.85     16409
          43       0.79      0.84      0.81      2713
          44       0.83      0.87      0.85       823
          45       0.82      0.82      0.82      3599
          46       0.91      0.86      0.89     10291
          47       0.25      0.78      0.38       499
          48       0.69      0.72      0.71       650
          49       0.87      0.89      0.88      5171
          50       0.54      0.79      0.64       688

    accuracy                           0.84    233000
   macro avg       0.79      0.84      0.80    233000
weighted avg       0.85      0.84      0.84    233000

195741 233000

test set report:
              precision    recall  f1-score   support

           0       0.83      0.82      0.82      2521
           1       0.89      0.89      0.89      1286
           2       0.85      0.87      0.86      5490
           3       0.72      0.80      0.76      1037
           4       0.90      0.83      0.86     38191
           5       0.78      0.85      0.81      4322
           6       0.76      0.85      0.80      1267
           7       0.55      0.78      0.64       316
           8       0.90      0.86      0.88     20328
           9       0.79      0.83      0.81      4084
          10       0.87      0.87      0.87      2559
          11       0.86      0.88      0.87      2351
          12       0.86      0.84      0.85     11456
          13       0.87      0.86      0.86      3982
          14       0.86      0.83      0.84      2442
          15       0.82      0.87      0.84      2287
          16       0.69      0.76      0.72      1017
          17       0.92      0.86      0.89      4792
          18       0.86      0.89      0.88      1428
          19       0.78      0.78      0.78      2897
          20       0.87      0.86      0.87      4474
          21       0.88      0.89      0.89      6154
          22       0.76      0.86      0.81      2719
          23       0.81      0.82      0.81      1457
          24       0.74      0.77      0.76      2015
          25       0.90      0.91      0.90      1644
          26       0.69      0.82      0.75       563
          27       0.29      0.79      0.43       413
          28       0.70      0.85      0.77       793
          29       0.81      0.83      0.82      6539
          30       0.88      0.83      0.85      2238
          31       0.82      0.80      0.81     19614
          32       0.80      0.86      0.83      5002
          33       0.83      0.86      0.84       448
          34       0.69      0.82      0.75      2749
          35       0.86      0.83      0.84      3637
          36       0.78      0.85      0.81      2409
          37       0.86      0.83      0.84      8480
          38       0.74      0.87      0.80       752
          39       0.83      0.83      0.83      2945
          40       0.87      0.85      0.86       905
          41       0.75      0.77      0.76      2031
          42       0.85      0.86      0.85     16545
          43       0.78      0.83      0.80      2741
          44       0.79      0.85      0.82       773
          45       0.82      0.80      0.81      3560
          46       0.91      0.86      0.88     10291
          47       0.25      0.77      0.38       501
          48       0.68      0.74      0.71       666
          49       0.88      0.90      0.89      5261
          50       0.50      0.73      0.60       628

    accuracy                           0.84    233000
   macro avg       0.78      0.83      0.80    233000
weighted avg       0.85      0.84      0.84    233000

195751 233000
Train: 0.8359, Val: 0.8401, Test: 0.8401

