100%|██████████| 1278/1278 [43:44<00:00,  2.05s/it] 
Epoch: 001, Loss: 2.8297243118
 74%|███████▍  | 950/1278 [37:06<08:35,  1.57s/it]  Epoch: 002, Loss: 0.9951775074
100%|██████████| 1278/1278 [28:57<00:00,  1.36s/it]
Epoch: 003, Loss: 0.9143822193
100%|██████████| 1278/1278 [29:03<00:00,  1.36s/it]
Epoch: 004, Loss: 0.9270526767
100%|██████████| 1278/1278 [29:10<00:00,  1.37s/it]
Epoch: 005, Loss: 0.9674911499
100%|██████████| 1278/1278 [29:13<00:00,  1.37s/it]
Epoch: 006, Loss: 0.9336917400
100%|██████████| 1278/1278 [29:20<00:00,  1.38s/it]
Epoch: 007, Loss: 0.9327997565
100%|██████████| 1278/1278 [29:15<00:00,  1.37s/it]
Epoch: 008, Loss: 0.9220435023
100%|██████████| 1278/1278 [29:12<00:00,  1.37s/it]
Epoch: 009, Loss: 0.9043405652
100%|██████████| 1278/1278 [29:02<00:00,  1.36s/it]
Epoch: 010, Loss: 0.9310432076
100%|██████████| 1278/1278 [29:16<00:00,  1.37s/it]
Epoch: 011, Loss: 0.9629333019
100%|██████████| 1278/1278 [29:30<00:00,  1.39s/it]
Epoch: 012, Loss: 0.9392029047
100%|██████████| 1278/1278 [29:17<00:00,  1.38s/it]
Epoch: 013, Loss: 0.9987128377
100%|██████████| 1278/1278 [29:17<00:00,  1.38s/it]
Epoch: 014, Loss: 0.9523099661
100%|██████████| 1278/1278 [31:14<00:00,  1.47s/it]
Epoch: 015, Loss: 0.8903591633
100%|██████████| 1278/1278 [29:53<00:00,  1.40s/it]
Epoch: 016, Loss: 0.9301185012
100%|██████████| 1278/1278 [30:01<00:00,  1.41s/it]
Epoch: 017, Loss: 0.9695795774
100%|██████████| 1278/1278 [30:05<00:00,  1.41s/it]
Epoch: 018, Loss: 0.8834511638
100%|██████████| 1278/1278 [29:52<00:00,  1.40s/it]
Epoch: 019, Loss: 0.8969367743
100%|██████████| 1278/1278 [30:02<00:00,  1.41s/it]
Epoch: 020, Loss: 0.9123229384
100%|██████████| 1278/1278 [30:05<00:00,  1.41s/it]
Epoch: 021, Loss: 0.9364356995
100%|██████████| 1278/1278 [30:03<00:00,  1.41s/it]
Epoch: 022, Loss: 0.9140812159
100%|██████████| 1278/1278 [30:08<00:00,  1.42s/it]
Epoch: 023, Loss: 0.8913068175
100%|██████████| 1278/1278 [30:09<00:00,  1.42s/it]
Epoch: 024, Loss: 0.8880122900
100%|██████████| 1278/1278 [30:11<00:00,  1.42s/it]
Epoch: 025, Loss: 0.8939847946
100%|██████████| 1278/1278 [29:58<00:00,  1.41s/it]
Epoch: 026, Loss: 0.9285100698
100%|██████████| 1278/1278 [30:03<00:00,  1.41s/it]
Epoch: 027, Loss: 0.9154841304
100%|██████████| 1278/1278 [30:02<00:00,  1.41s/it]
Epoch: 028, Loss: 0.9048448205
100%|██████████| 1278/1278 [30:11<00:00,  1.42s/it]
Epoch: 029, Loss: 0.9011312127
100%|██████████| 1278/1278 [30:13<00:00,  1.42s/it]
Epoch: 030, Loss: 0.9246854186
100%|██████████| 1278/1278 [30:11<00:00,  1.42s/it]
Epoch: 031, Loss: 0.8570652604
100%|██████████| 1278/1278 [29:59<00:00,  1.41s/it]
Epoch: 032, Loss: 0.8923781514
100%|██████████| 1278/1278 [30:02<00:00,  1.41s/it]
Epoch: 033, Loss: 1.0038312674
100%|██████████| 1278/1278 [30:10<00:00,  1.42s/it]
Epoch: 034, Loss: 0.8774220943
100%|██████████| 1278/1278 [30:06<00:00,  1.41s/it]
Epoch: 035, Loss: 0.8703173995
100%|██████████| 1278/1278 [30:15<00:00,  1.42s/it]
Epoch: 036, Loss: 0.8854181170
100%|██████████| 1278/1278 [30:07<00:00,  1.41s/it]
Epoch: 037, Loss: 0.9033335447
100%|██████████| 1278/1278 [30:16<00:00,  1.42s/it]
Epoch: 038, Loss: 0.8696723580
100%|██████████| 1278/1278 [30:07<00:00,  1.41s/it]
Epoch: 039, Loss: 0.8940966725
100%|██████████| 1278/1278 [30:15<00:00,  1.42s/it]
Epoch: 040, Loss: 0.8713529706
100%|██████████| 1278/1278 [30:09<00:00,  1.42s/it]
Epoch: 041, Loss: 0.9026616812
100%|██████████| 1278/1278 [30:17<00:00,  1.42s/it]
Epoch: 042, Loss: 0.9178897738
100%|██████████| 1278/1278 [30:09<00:00,  1.42s/it]
Epoch: 043, Loss: 0.9736866951
100%|██████████| 1278/1278 [30:17<00:00,  1.42s/it]
Epoch: 044, Loss: 0.8721383810
100%|██████████| 1278/1278 [30:10<00:00,  1.42s/it]
Epoch: 045, Loss: 0.8768763542
100%|██████████| 1278/1278 [30:16<00:00,  1.42s/it]
Epoch: 046, Loss: 0.8616346717
100%|██████████| 1278/1278 [30:10<00:00,  1.42s/it]
Epoch: 047, Loss: 0.8664025664
100%|██████████| 1278/1278 [30:15<00:00,  1.42s/it]
Epoch: 048, Loss: 0.8749117851
100%|██████████| 1278/1278 [30:10<00:00,  1.42s/it]
Epoch: 049, Loss: 0.9251807332
100%|██████████| 1278/1278 [30:15<00:00,  1.42s/it]
Epoch: 050, Loss: 0.8637599945
100%|██████████| 1278/1278 [30:09<00:00,  1.42s/it]
Epoch: 051, Loss: 0.9058557153
100%|██████████| 1278/1278 [30:16<00:00,  1.42s/it]
Epoch: 052, Loss: 0.8728345037
100%|██████████| 1278/1278 [30:11<00:00,  1.42s/it]
Epoch: 053, Loss: 0.9046033025
100%|██████████| 1278/1278 [30:20<00:00,  1.42s/it]
Epoch: 054, Loss: 0.8695942163
100%|██████████| 1278/1278 [30:27<00:00,  1.43s/it]
Epoch: 055, Loss: 0.8552818298
100%|██████████| 1278/1278 [31:02<00:00,  1.46s/it]
Epoch: 056, Loss: 0.8782889247
100%|██████████| 1278/1278 [31:06<00:00,  1.46s/it]
Epoch: 057, Loss: 0.8924760818
100%|██████████| 1278/1278 [34:02<00:00,  1.60s/it]
Epoch: 058, Loss: 0.8884249926
100%|██████████| 1278/1278 [31:18<00:00,  1.47s/it]
Epoch: 059, Loss: 0.8796043396
100%|██████████| 1278/1278 [35:40<00:00,  1.67s/it]
Epoch: 060, Loss: 0.9149559736
100%|██████████| 1278/1278 [37:09<00:00,  1.74s/it] 
Epoch: 061, Loss: 0.8719408512
100%|██████████| 1278/1278 [36:23<00:00,  1.71s/it] 
Epoch: 062, Loss: 0.8743208051
100%|██████████| 1278/1278 [32:08<00:00,  1.51s/it]
Epoch: 063, Loss: 0.8642781973
100%|██████████| 1278/1278 [32:06<00:00,  1.51s/it]
Epoch: 064, Loss: 0.8655178547
100%|██████████| 1278/1278 [32:13<00:00,  1.51s/it]
Epoch: 065, Loss: 0.9275757074
100%|██████████| 1278/1278 [32:20<00:00,  1.52s/it]
Epoch: 066, Loss: 0.8667910099
100%|██████████| 1278/1278 [32:10<00:00,  1.51s/it]
Epoch: 067, Loss: 0.8831492662
100%|██████████| 1278/1278 [32:09<00:00,  1.51s/it]
Epoch: 068, Loss: 0.8902737498
100%|██████████| 1278/1278 [31:54<00:00,  1.50s/it]
Epoch: 069, Loss: 0.8699550629
100%|██████████| 1278/1278 [32:06<00:00,  1.51s/it]
Epoch: 070, Loss: 0.8594564199
100%|██████████| 1278/1278 [32:06<00:00,  1.51s/it]
Epoch: 071, Loss: 0.8698028326
100%|██████████| 1278/1278 [32:08<00:00,  1.51s/it]
Epoch: 072, Loss: 0.8836667538
100%|██████████| 1278/1278 [31:48<00:00,  1.49s/it]
Epoch: 073, Loss: 0.9035757184
100%|██████████| 1278/1278 [32:07<00:00,  1.51s/it]
Epoch: 074, Loss: 0.8834276795
100%|██████████| 1278/1278 [32:02<00:00,  1.50s/it]
Epoch: 075, Loss: 0.8686919808
100%|██████████| 1278/1278 [32:12<00:00,  1.51s/it]
Epoch: 076, Loss: 0.8624784350
100%|██████████| 1278/1278 [32:11<00:00,  1.51s/it]
Epoch: 077, Loss: 0.8522157669
100%|██████████| 1278/1278 [31:47<00:00,  1.49s/it]
Epoch: 078, Loss: 0.8777232170
100%|██████████| 1278/1278 [31:51<00:00,  1.50s/it]
Epoch: 079, Loss: 0.8861445785
100%|██████████| 1278/1278 [32:32<00:00,  1.53s/it]
Epoch: 080, Loss: 0.8941676617
100%|██████████| 1278/1278 [31:55<00:00,  1.50s/it]
Epoch: 081, Loss: 0.8879355192
100%|██████████| 1278/1278 [32:05<00:00,  1.51s/it]
Epoch: 082, Loss: 0.9194251299
100%|██████████| 1278/1278 [32:10<00:00,  1.51s/it]
Epoch: 083, Loss: 0.8841319084
100%|██████████| 1278/1278 [32:23<00:00,  1.52s/it]
Epoch: 084, Loss: 0.9297059774
100%|██████████| 1278/1278 [32:21<00:00,  1.52s/it]
Epoch: 085, Loss: 0.8565624952
100%|██████████| 1278/1278 [32:19<00:00,  1.52s/it]
Epoch: 086, Loss: 0.8924462795
100%|██████████| 1278/1278 [32:13<00:00,  1.51s/it]
Epoch: 087, Loss: 0.8649401069
100%|██████████| 1278/1278 [34:10<00:00,  1.60s/it]
Epoch: 088, Loss: 0.8622478843
100%|██████████| 1278/1278 [42:58<00:00,  2.02s/it] 
Epoch: 089, Loss: 0.8976433277
100%|██████████| 1278/1278 [35:00<00:00,  1.64s/it]
Epoch: 090, Loss: 0.9325999618
100%|██████████| 1278/1278 [33:17<00:00,  1.56s/it]
Epoch: 091, Loss: 0.9679055214
100%|██████████| 1278/1278 [33:23<00:00,  1.57s/it]
Epoch: 092, Loss: 0.8919984698
100%|██████████| 1278/1278 [33:17<00:00,  1.56s/it]
Epoch: 093, Loss: 0.8568074107
100%|██████████| 1278/1278 [33:36<00:00,  1.58s/it]
Epoch: 094, Loss: 0.8644782901
100%|██████████| 1278/1278 [33:30<00:00,  1.57s/it]
Epoch: 095, Loss: 0.8873441219
100%|██████████| 1278/1278 [33:33<00:00,  1.58s/it]
Epoch: 096, Loss: 0.8684967160
100%|██████████| 1278/1278 [33:35<00:00,  1.58s/it]
Epoch: 097, Loss: 0.8768581748
100%|██████████| 1278/1278 [33:36<00:00,  1.58s/it]
Epoch: 098, Loss: 0.8659259081
100%|██████████| 1278/1278 [33:23<00:00,  1.57s/it]Epoch: 099, Loss: 0.8604760170
min_loss tensor(0.8522, device='cuda:0', grad_fn=<DivBackward0>)

Validation set:
100%|██████████| 73/73 [03:19<00:00,  2.73s/it]
              precision    recall  f1-score   support

           0       0.88      0.83      0.85      6036
           1       0.94      0.88      0.91      1856
           2       0.89      0.87      0.88     10846
           3       0.88      0.84      0.86      3687
           4       0.75      0.84      0.79     86878
           5       0.86      0.87      0.87     12794
           6       0.87      0.84      0.86      5085
           7       0.73      0.53      0.61       563
           8       0.87      0.88      0.87     40878
           9       0.86      0.85      0.85     13758
          10       0.92      0.81      0.86      2670
          11       0.87      0.82      0.84      3261
          12       0.83      0.76      0.79     25949
          13       0.89      0.84      0.86      9639
          14       0.89      0.84      0.86      6018
          15       0.86      0.86      0.86      6505
          16       0.85      0.75      0.80      3798
          17       0.91      0.85      0.88      6597
          18       0.87      0.89      0.88      4002
          19       0.83      0.83      0.83      8633
          20       0.88      0.87      0.88     14210
          21       0.90      0.88      0.89     17771
          22       0.85      0.86      0.85      9459
          23       0.84      0.79      0.82      2793
          24       0.84      0.78      0.81      6487
          25       0.91      0.87      0.89      2906
          26       0.86      0.85      0.85      2751
          27       0.87      0.79      0.82      4317
          28       0.87      0.78      0.83      3223
          29       0.85      0.82      0.83     12079
          30       0.90      0.75      0.82      3119
          31       0.75      0.72      0.73     54021
          32       0.87      0.86      0.87     18153
          33       0.86      0.84      0.85      1194
          34       0.84      0.86      0.85     16647
          35       0.88      0.80      0.84      5830
          36       0.86      0.90      0.88      9447
          37       0.86      0.79      0.82     23570
          38       0.88      0.81      0.84      2070
          39       0.88      0.85      0.87      7506
          40       0.90      0.83      0.87      1660
          41       0.85      0.86      0.85      8720
          42       0.75      0.80      0.78     51732
          43       0.85      0.79      0.82      4513
          44       0.85      0.84      0.84      1877
          45       0.87      0.84      0.86     13394
          46       0.86      0.87      0.86     16248
          47       0.53      0.09      0.15       436
          48       0.80      0.74      0.77      1756
          49       0.90      0.88      0.89     11337
          50       0.89      0.73      0.80      1321

    accuracy                           0.83    590000
   macro avg       0.85      0.81      0.83    590000
weighted avg       0.83      0.83      0.83    590000

487140 590000
0.8256610169491525
Testing set:
100%|██████████| 7/7 [00:16<00:00,  2.29s/it]
              precision    recall  f1-score   support

           0       0.94      0.84      0.89      1000
           1       0.99      0.87      0.92      1000
           2       0.93      0.87      0.90      1000
           3       0.97      0.80      0.88      1000
           4       0.22      0.84      0.35      1000
           5       0.78      0.88      0.82      1000
           6       0.96      0.86      0.91      1000
           7       0.99      0.55      0.71      1000
           8       0.67      0.87      0.76      1000
           9       0.87      0.88      0.88      1000
          10       0.99      0.82      0.90      1000
          11       0.95      0.81      0.87      1000
          12       0.68      0.79      0.73      1000
          13       0.92      0.85      0.89      1000
          14       0.91      0.84      0.87      1000
          15       0.89      0.86      0.87      1000
          16       0.97      0.75      0.85      1000
          17       0.95      0.82      0.88      1000
          18       0.95      0.88      0.92      1000
          19       0.75      0.82      0.78      1000
          20       0.80      0.87      0.83      1000
          21       0.90      0.88      0.89      1000
          22       0.83      0.84      0.84      1000
          23       0.98      0.80      0.88      1000
          24       0.91      0.78      0.84      1000
          25       0.97      0.85      0.91      1000
          26       0.97      0.86      0.91      1000
          27       0.97      0.79      0.87      1000
          28       0.94      0.79      0.86      1000
          29       0.91      0.83      0.87      1000
          30       0.97      0.73      0.84      1000
          31       0.37      0.70      0.48      1000
          32       0.83      0.88      0.85      1000
          33       0.98      0.82      0.89      1000
          34       0.82      0.87      0.85      1000
          35       0.94      0.81      0.87      1000
          36       0.92      0.87      0.89      1000
          37       0.73      0.80      0.76      1000
          38       0.98      0.79      0.88      1000
          39       0.96      0.86      0.91      1000
          40       0.97      0.83      0.89      1000
          41       0.92      0.84      0.88      1000
          42       0.37      0.79      0.50      1000
          43       0.93      0.78      0.84      1000
          44       0.98      0.83      0.90      1000
          45       0.81      0.84      0.83      1000
          46       0.80      0.87      0.83      1000
          47       1.00      0.07      0.13      1000
          48       0.99      0.72      0.84      1000
          49       0.93      0.88      0.91      1000
          50       0.98      0.70      0.82      1000

    accuracy                           0.81     51000
   macro avg       0.88      0.81      0.82     51000
weighted avg       0.88      0.81      0.82     51000

41101 51000
0.8059019607843138


total_labeled_loader = NeighborLoader(data, num_neighbors=[-1] * 1, batch_size=2)

Total set:
100%|██████████| 2936698/2936698 [35:03<00:00, 1396.02it/s]
              precision    recall  f1-score   support

           0       0.88      0.83      0.86     60612
           1       0.95      0.84      0.89     19254
           2       0.88      0.88      0.88    108796
           3       0.89      0.83      0.86     37737
           4       0.80      0.80      0.80    861111
           5       0.83      0.89      0.86    126983
           6       0.86      0.84      0.85     52260
           7       0.98      0.05      0.09      5938
           8       0.83      0.89      0.86    403224
           9       0.84      0.86      0.85    138608
          10       0.93      0.80      0.86     27912
          11       0.89      0.81      0.85     33731
          12       0.76      0.79      0.77    257138
          13       0.89      0.83      0.86     96803
          14       0.89      0.82      0.85     60499
          15       0.55      0.88      0.67     66926
          16       0.85      0.72      0.78     38858
          17       0.90      0.85      0.87     65060
          18       0.87      0.89      0.88     40326
          19       0.80      0.83      0.82     85562
          20       0.84      0.89      0.87    143152
          21       0.88      0.89      0.89    174456
          22       0.85      0.84      0.84     93736
          23       0.88      0.77      0.82     28109
          24       0.72      0.80      0.76     65606
          25       0.91      0.87      0.89     29376
          26       0.90      0.75      0.82     28074
          27       0.87      0.79      0.83     43474
          28       0.88      0.65      0.75     31843
          29       0.83      0.83      0.83    119155
          30       0.90      0.75      0.82     31975
          31       0.79      0.66      0.72    532829
          32       0.86      0.87      0.86    178591
          33       0.90      0.79      0.84     13335
          34       0.84      0.86      0.85    166758
          35       0.65      0.82      0.72     57946
          36       0.85      0.90      0.87     94016
          37       0.83      0.80      0.82    231476
          38       0.89      0.79      0.84     21248
          39       0.88      0.86      0.87     73909
          40       0.91      0.82      0.86     17053
          41       0.83      0.87      0.85     87676
          42       0.74      0.80      0.77    510716
          43       0.86      0.80      0.83     45488
          44       0.85      0.84      0.85     20180
          45       0.85      0.86      0.85    133545
          46       0.88      0.87      0.87    161601
          47       0.69      0.00      0.00      5306
          48       0.81      0.75      0.78     18387
          49       0.89      0.88      0.89    113541
          50       0.94      0.66      0.78     13500

    accuracy                           0.82   5873395
   macro avg       0.85      0.79      0.80   5873395
weighted avg       0.82      0.82      0.82   5873395

4799555 5873395
0.8171687754697241

total_labeled_loader = NeighborLoader(data, num_neighbors=[-1] * 2, batch_size=1)
Total set:
100%|██████████| 5873395/5873395 [3:40:11<00:00, 444.56it/s]  
              precision    recall  f1-score   support

           0       0.88      0.82      0.85     60612
           1       0.93      0.87      0.90     19254
           2       0.89      0.86      0.88    108796
           3       0.88      0.84      0.86     37737
           4       0.74      0.83      0.78    861111
           5       0.86      0.87      0.86    126983
           6       0.87      0.83      0.85     52260
           7       0.74      0.53      0.62      5938
           8       0.86      0.87      0.87    403224
           9       0.86      0.84      0.85    138608
          10       0.93      0.80      0.86     27912
          11       0.88      0.81      0.84     33731
          12       0.81      0.76      0.78    257138
          13       0.89      0.83      0.86     96803
          14       0.88      0.83      0.85     60499
          15       0.87      0.85      0.86     66926
          16       0.85      0.74      0.79     38858
          17       0.90      0.84      0.87     65060
          18       0.88      0.89      0.88     40326
          19       0.83      0.82      0.82     85562
          20       0.88      0.87      0.87    143152
          21       0.91      0.88      0.89    174456
          22       0.84      0.85      0.85     93736
          23       0.85      0.79      0.82     28109
          24       0.84      0.77      0.80     65606
          25       0.91      0.86      0.88     29376
          26       0.86      0.84      0.85     28074
          27       0.87      0.78      0.82     43474
          28       0.86      0.76      0.81     31843
          29       0.85      0.81      0.83    119155
          30       0.90      0.75      0.82     31975
          31       0.72      0.71      0.72    532829
          32       0.87      0.86      0.86    178591
          33       0.88      0.82      0.85     13335
          34       0.84      0.85      0.85    166758
          35       0.89      0.79      0.84     57946
          36       0.87      0.88      0.87     94016
          37       0.85      0.79      0.82    231476
          38       0.88      0.78      0.83     21248
          39       0.88      0.85      0.87     73909
          40       0.90      0.83      0.87     17053
          41       0.85      0.85      0.85     87676
          42       0.74      0.79      0.77    510716
          43       0.86      0.78      0.82     45488
          44       0.86      0.83      0.85     20180
          45       0.87      0.83      0.85    133545
          46       0.85      0.87      0.86    161601
          47       0.52      0.08      0.14      5306
          48       0.81      0.74      0.77     18387
          49       0.90      0.87      0.89    113541
          50       0.89      0.71      0.79     13500

    accuracy                           0.82   5873395
   macro avg       0.85      0.80      0.82   5873395
weighted avg       0.82      0.82      0.82   5873395

4811037 5873395
0.8191236925151467